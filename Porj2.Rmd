---
title: "Project2"
author: "Yiliang Yuan & Yiyu Lin"
date: "2022-11-26"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggplot2)
library(gridExtra)
library(GGally)
library(reshape2)
library(ROCR)
library(cvAUC)
library(dplyr)
library(pROC)
library(randomForest)
```

***
## 1 Data Collection and Exploration
## (a)
Global climate change is an important scientific and public topic. In this topic, there’s a prediction that, in the Arctic area, surface air temperatures will have the strongest dependency on increasing atmospheric carbon dioxide levels. In further study of this dependency, the key is to detect the cloud condition. This is a challenging problem if we apply two previous MISR operational algorithms because of the similarities between the remote sensing image of clouds and that of ice- or snow-covered surface. Thus, based on the new idea of searching for cloud-free surface, the goal of this study is to develop a computationally efficient algorithm with the combination of classification and clustering methods, but without the requiring human intervention
The data in this study represents 10 MISR orbits from Path 26, which is 360 km wide, and starting at the Arctic until Antarctica. The time between each orbit is 16 days. In this path, there’re 180 blocks to collect data, but for each orbit case, only 6 blocks’ data will be taken. In these 60 blocks, the author removed 3 blocks whose cloud condition can be detected by the previous MISR algorithm. Therefore, there are 57 blocks data units with 7,114,248 1.1-km resolution pixels with 36 radiation measurements for each pixel in this study. Author hand-labeled these data, with “1” for cloudy, “-1” for not cloudy, and “0” for ambiguous.
There’re also three key features used in this study to differentiate surface pixels from cloudy pixels. The first is CORR. It is used to differentiate MISR images of the same scene from different MISR viewing directions. The second is SDAn, the standard deviation of MISR nadir camera pixel values across a scene. The last one, NDAI, is a normalized difference angular index. It can be used to characterize the changes in a scene with changes in the MISR view direction.
In conclusion, both the newly-developed ELCM and ELCM-QDA algorithm return a result with better accuracy and better coverage, when compared to the results generated from previous MISR algorithm. Moreover, the ELCM-QDA algorithm can generate more information and perform consistently on probability prediction. The impact of this work is significant. Statisticians will play a more important role in data analyses and statistical thinking will take more responsibility in solving modern scientific problems.



## (b)
```{r}
image1=read.csv('imagem1.txt',sep='',header=FALSE)
image2=read.csv('imagem2.txt', sep='',header=FALSE)
image3=read.csv('imagem3.txt', sep='',header=FALSE)
colnames(image1) = c('y','x','expert_label','NDAI','SD','CORR','DF','CF','BF','AF','AN')
colnames(image2) = c('y','x','expert_label','NDAI','SD','CORR','DF','CF','BF','AF','AN')
colnames(image3) = c('y','x','expert_label','NDAI','SD','CORR','DF','CF','BF','AF','AN')
image <- rbind(image1,image2,image3)
```

First of all, we can find the percentage of pixels for the different classes in different images as:
```{r}
per1 <- image1 %>%
  group_by(expert_label)%>%
  summarise(image1 = round(n()/nrow(.),3))
per2 <- image2 %>%
  group_by(expert_label)%>%
  summarise(image2 = round(n()/nrow(.),3))
per3 <- image3 %>%
  group_by(expert_label)%>%
  summarise(image3 = round(n()/nrow(.),3))
per_total <- image %>%
  group_by(expert_label)%>%
  summarise(image_total = round(n()/nrow(.),3))
per_combined <- list(per1,per2,per3,per_total)
per_combined %>% reduce(full_join, by='expert_label')
```
Then, we can plot the maps of different images by x,y corrdinates values with expert labels as colors as:
```{r}
myColors = c("gray40","black","white")
p1 = ggplot(image1, aes(x, y, colour = factor(expert_label))) + 
  geom_point(show.legend = FALSE)+
  scale_color_manual(values=myColors)+
  labs(colour="expert_label")
p2 = ggplot(image2, aes(x, y, colour = factor(expert_label))) + 
  geom_point(show.legend = FALSE)+
  scale_color_manual(values=myColors)+
  labs(colour="expert_label")
p3 = ggplot(image3, aes(x, y, colour = factor(expert_label))) + 
  geom_point(show.legend = FALSE)+
  scale_color_manual(values=myColors)+
  labs(colour="expert_label")
```
    
```{r,fig.width=8,fig.height=3}
gg_legend <- ggplot(image1, aes(x, y, colour = factor(expert_label))) + 
  geom_point()+
  scale_color_manual(values=myColors)+
  labs(colour="expert_label") +
  theme(legend.position = "bottom")
extract_legend <- function(my_ggp) {
  step1 <- ggplot_gtable(ggplot_build(my_ggp))
  step2 <- which(sapply(step1$grobs, function(x) x$name) == "guide-box")
  step3 <- step1$grobs[[step2]]
  return(step3)
}
shared_legend <- extract_legend(gg_legend)
grid.arrange(arrangeGrob(p1,p2,p3,ncol=3), shared_legend,nrow = 2, heights = c(10, 1))
```
In the graphs above, we plot each pixels as a single dot in the graph, but they can be merged to a large area with the same color. This means pixels from each class tend to be surrounded by the pixels from the same class. Thus, for all the dataset, we cannot treat samples or pixels as i.i.d. 


## (c)
```{r}
image = image %>%
  mutate(expert_label = factor(expert_label))
image_cor = round(cor(image[,4:11]),2)
get_upper_tri <- function(corr){
    corr[lower.tri(corr)]<- NA
    return(corr)
}
upper_tri = get_upper_tri(image_cor)
melted_image_corr <- melt(upper_tri, na.rm = TRUE)

heatmap <- ggplot(data = melted_image_corr, aes(Var2, Var1, fill = value))+
 geom_tile(color = "white")+
 scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
   midpoint = 0, limit = c(-1,1), space = "Lab", 
   name="Correlations") +
  theme_minimal()+ 
 theme(axis.text.x = element_text(angle = 45, vjust = 1, 
    size = 12, hjust = 1))+
 coord_fixed()

heatmap + 
geom_text(aes(Var2, Var1, label = value), color = "black", size = 4) +
theme(
  axis.title.x = element_blank(),
  axis.title.y = element_blank(),
  panel.grid.major = element_blank(),
  panel.border = element_blank(),
  panel.background = element_blank(),
  axis.ticks = element_blank(),
  legend.justification = c(1, 0),
  legend.position = c(0.6, 0.7),
  legend.direction = "horizontal")+
  guides(fill = guide_colorbar(barwidth = 7, barheight = 1,
                title.position = "top", title.hjust = 0.5))
```
Since the size of our dataset is too large and there are 7 variables to pair, it might not be a good idea to use pairs plot here. Instead, we can draw a heat map for the correlations between the features themselves, and the result is very interesting. NDAI, SD and CORR are positively correlated, and all the radiance variables are also positively correlated. However, radiance and other features(CORR,NDAI,SD) are negatively correlated. In addition, we find that radiance angle AN, AF, BF, CF are strongly correlated since their correlations are all larger than 0.75. Radiance angle DF is strongly correlated to radiance angle CF and BF, but its correlations with radiance angle AF and AN are not that strong.

```{r}
g1 = ggplot(image,aes(x=expert_label, y=NDAI))+
  geom_boxplot()
g2 = ggplot(image,aes(x=expert_label, y=SD))+
  geom_boxplot()
g3 = ggplot(image,aes(x=expert_label, y=CORR))+
  geom_boxplot()
g4 = ggplot(image,aes(x=expert_label, y=DF))+
  geom_boxplot()
g5 = ggplot(image,aes(x=expert_label, y=CF))+
  geom_boxplot()
g6 = ggplot(image,aes(x=expert_label, y=BF))+
  geom_boxplot()
g7 = ggplot(image,aes(x=expert_label, y=AF))+
  geom_boxplot()
g8 = ggplot(image,aes(x=expert_label, y=AN))+
  geom_boxplot()
```

```{r,fig.width=9,fig.height=3}
grid.arrange(g1,g2,g3,ncol=3)
```
By plotting the boxplots of NDAI, SD, and CORR with expert label as x-axis, we find that cloud class tends to have higher values of NDAI, SD and CORR.
```{r,fig.width=9,fig.height=6}
grid.arrange(g4,g5,g6,g7,g8,ncol=3)
```
Again, by plotting boxplots of DF, CF, BF, AF and AN with expert labels as x-axis, we find that cloud class tends to have smaller values of radiance in different angles except that in DF. For radiance angle DF, all the classes have similar distributions of radiance values, so it is hard to tell differences between cloud and no cloud classes.


# 2 Preparation

## (a)

### First Splitting method
For our first method, we want to use a simple and direct way to split our data. In order to have enough data for training, we can use the data of two pictures as our training set so that the training set takes 66.6% of the entire data. Then, we only need to split the data of image3 into a validation and test set. Here, we set the ratio of the amount of data in three sets to be around 6:2:1, so validation set should take about 22.2% of the entire data and test set should take 11.1% of the entire data. We first set two images to be our training set, and split the remaining picture into three rectangles of equal size. Then, we can take two rectangles as our validation set and one as test set. In this method, we need to make sure that each rectangle we split should contain data from different classes, or be diversified. After checking the three images we plotted in 1(b), we decide to use the first two images as our training set, and split the third one. In the third image, we can set thresholds values for the y-axis, and divide the image into three rectangles accroding to these thresholds values.
```{r}
image3_filtered = image3 %>% filter(expert_label!=0) %>% mutate(expert_label=factor(expert_label))
y_max_p3 = max(image3_filtered$y)
y_min_p3 = min(image3_filtered$y)
lim1 = y_max_p3-(y_max_p3-y_min_p3)/3
lim2 = y_max_p3-(y_max_p3-y_min_p3)/3*2
ggplot(image3_filtered,aes(x,y,color=expert_label))+
  geom_point()+
  scale_color_manual(values=c("grey40","blue"))+
  labs(colour="expert_label")+
  geom_hline(yintercept=lim1,color="red")+
  geom_hline(yintercept = lim2,color="red")
```
From the plot above, we can find that our thresholds for y-axis divide the image into three rectangles containing data from different classes. We can pick the one on the top to be our test set, and the remaining two to be our validation set.
```{r}
train_method1 = rbind(image1,image2) %>% filter(expert_label!=0) %>% mutate(expert_label=factor(expert_label))
test_method1 = image3_filtered[image3_filtered$y>=lim1,]
validation_method1 = image3_filtered[image3_filtered$y<lim1,]
```

### Second Method
In the second method, we can use blocks to split the all the images and randomly select 70% blocks as our training set, 20% as validation set, and the remaining 10% as test set. We plan to divide each image into 100 blocks and then we will have totally 300 blocks. Then, we can divide those 300 blocks into train, validation, and test sets according to the ratio we set.
```{r}
set.seed(111)
image1_xcut = seq(min(image1$x),max(image1$x),length=11)
image2_xcut = seq(min(image2$x),max(image2$x),length=11)
image3_xcut = seq(min(image3$x),max(image3$x),length=11)
image1_ycut = seq(min(image1$y),max(image1$y),length=11)
image2_ycut = seq(min(image2$y),max(image2$y),length=11)
image3_ycut = seq(min(image3$y),max(image3$y),length=11)

blocks = list()
b_lab = 1
for(i in 1:10){
  for(j in 1:10){
    single_block = image1 %>% filter(x>image1_xcut[i] & x<image1_xcut[i+1] & y>image1_ycut[i] & y<image1_ycut[i+1])
    blocks[[b_lab]] = single_block
    b_lab = b_lab+1
  }
}
for(i in 1:10){
  for(j in 1:10){
    single_block = image2 %>% filter(x>image2_xcut[i] & x<image2_xcut[i+1] & y>image2_ycut[i] & y<image2_ycut[i+1])
    blocks[[b_lab]] = single_block
    b_lab = b_lab+1
  }
}
for(i in 1:10){
  for(j in 1:10){
    single_block = image3 %>% filter(x>image3_xcut[i] & x<image3_xcut[i+1] & y>image3_ycut[i] & y<image3_ycut[i+1])
    blocks[[b_lab]] = single_block
    b_lab = b_lab+1
  }
}
sample_indexs = 1:300
train_index = sample(sample_indexs,size = 210,replace = FALSE)
validation_index = sample(sample_indexs[-train_index],size=60,replace = FALSE)
test_index = sample_indexs[-c(train_index,validation_index)]

train_method2 = blocks[[train_index[1]]]
validation_method2 = blocks[[validation_index[1]]]
test_method2 = blocks[[test_index[1]]]
for(i in train_index[-1]){
  train_method2 = rbind(train_method2,blocks[[i]])
}
for(i in validation_index[-1]){
  validation_method2 = rbind(validation_method2,blocks[[i]])
}
for(i in test_index[-1]){
  test_method2 = rbind(test_method2,blocks[[i]])
}
train_method2 = train_method2%>%filter(expert_label!=0)%>%mutate(expert_label=factor(expert_label))
test_method2 = test_method2%>%filter(expert_label!=0)%>%mutate(expert_label=factor(expert_label))
validation_method2 = validation_method2%>%filter(expert_label!=0)%>%mutate(expert_label=factor(expert_label))
```


## (b)
The accuracy of the trivial classifier on the validation and test set of two methods is:
```{r}
# accuracy of method1
accuracy = matrix(NA,nrow=2,ncol=2,dimnames=list(c("validation","test"),c("method1","method2")))
accuracy[1,1] = sum(validation_method1$expert_label==-1)/nrow(validation_method1)
accuracy[1,2] = sum(validation_method2$expert_label==-1)/nrow(validation_method2)
accuracy[2,1] = sum(test_method1$expert_label==-1)/nrow(test_method1)
accuracy[2,2] = sum(test_method2$expert_label==-1)/nrow(test_method2)
accuracy
```
This kind of trivial classifier will have high accuracy when a dataset consist almost entirely of cloud-free class. On the table above, we can find that the trivial classifier does not perform well on most of the sets. For the validation set of our method1, since cloud-free class occupies a large proportion of the data, the accuracy of trivial classifier is pretty high compared with other sets, but the accuracy is still only around 0.7. Therefore, we would say our classification problem is not trivial.

## (c)
In order to determine 3 best features out of 8 features, we firstly used random forest to find and plot the importance of each feature based on train sets of both methods. We found that NDAI, SD and CORR were the features having the greatest importance in both plots. Then, we decided to use each feature to build a single-predictor logistics regression model based on training data set from both methods, and use the validation data sets from both methods to draw the ROC curve.
We got two area under the ROC curve (AUC) values, and made them equal weighted to get the average AUC value to evaluate the overall performance of each model. 
An ideal ROC curve will have huge top left corner, so the larger the average AUC value the better the classifier. Again, NDAI, SD and CORR had the largest average AUC values. So, combined with the result of random forest, we decided to use NDAI, SD, and CORR as our three "best" features. Among them, NDAI performed the best in both random forest and ROC curves, so it should be the most useful variable for our classification.

```{r}
# change expert_label's not cloudy label from -1 to 0
temp_train_method1=train_method1 %>% 
   mutate(expert_label = ifelse(expert_label == -1,0,1))

temp_train_method2=train_method2 %>%
  mutate(expert_label = ifelse(expert_label == -1,0,1))

temp_validation_method1=validation_method1 %>%
  mutate(expert_label = ifelse(expert_label == -1,0,1))

temp_validation_method2=validation_method2 %>%
  mutate(expert_label = ifelse(expert_label == -1,0,1))
```

```{r}
cloud_RF1 = randomForest(as.factor(expert_label)~NDAI+SD+CORR+AF+BF+CF+DF+AN,data=temp_train_method1,ntree=100)
cloud_RF2 = randomForest(as.factor(expert_label)~NDAI+SD+CORR+AF+BF+CF+DF+AN,data=temp_train_method2,ntree=100)
varImpPlot(cloud_RF1)
varImpPlot(cloud_RF2)
```

```{r}
logm1_1=glm(expert_label~NDAI,data=temp_train_method1,family = "binomial")
logm1_2=glm(expert_label~SD,data=temp_train_method1,family = "binomial")
logm1_3=glm(expert_label~CORR,data=temp_train_method1,family = "binomial")
logm1_4=glm(expert_label~DF,data=temp_train_method1,family = "binomial")
logm1_5=glm(expert_label~CF,data=temp_train_method1,family = "binomial")
logm1_6=glm(expert_label~BF,data=temp_train_method1,family = "binomial")
logm1_7=glm(expert_label~AF,data=temp_train_method1,family = "binomial")
logm1_8=glm(expert_label~AN,data=temp_train_method1,family = "binomial")
```
```{r}
m1_auc=c()
prob=predict(logm1_1,temp_validation_method1,type='response')
m1_auc[1]=auc(temp_validation_method1$expert_label, prob)
prob=predict(logm1_2,temp_validation_method1,type='response')
m1_auc[2]=auc(temp_validation_method1$expert_label, prob)

prob=predict(logm1_3,temp_validation_method1,type='response')
m1_auc[3]=auc(temp_validation_method1$expert_label, prob)
prob=predict(logm1_4,temp_validation_method1,type='response')
m1_auc[4]=auc(temp_validation_method1$expert_label, prob)

prob=predict(logm1_5,temp_validation_method1,type='response')
m1_auc[5]=auc(temp_validation_method1$expert_label, prob)
prob=predict(logm1_6,temp_validation_method1,type='response')
m1_auc[6]=auc(temp_validation_method1$expert_label, prob)

prob=predict(logm1_7,temp_validation_method1,type='response')
m1_auc[7]=auc(temp_validation_method1$expert_label, prob)
prob=predict(logm1_8,temp_validation_method1,type='response')
m1_auc[8]=auc(temp_validation_method1$expert_label, prob)
```

```{r}
logm2_1=glm(expert_label~NDAI,data=temp_train_method2,family = "binomial")
logm2_2=glm(expert_label~SD,data=temp_train_method2,family = "binomial")
logm2_3=glm(expert_label~CORR,data=temp_train_method2,family = "binomial")
logm2_4=glm(expert_label~DF,data=temp_train_method2,family = "binomial")
logm2_5=glm(expert_label~CF,data=temp_train_method2,family = "binomial")
logm2_6=glm(expert_label~BF,data=temp_train_method2,family = "binomial")
logm2_7=glm(expert_label~AF,data=temp_train_method2,family = "binomial")
logm2_8=glm(expert_label~AN,data=temp_train_method2,family = "binomial")
```
```{r}
library(ROCR)
m2_auc=c()
prob=predict(logm2_1,temp_validation_method2,type='response')
m2_auc[1]=auc(temp_validation_method2$expert_label, prob)
prob=predict(logm2_2,temp_validation_method2,type='response')
m2_auc[2]=auc(temp_validation_method2$expert_label, prob)

prob=predict(logm2_3,temp_validation_method2,type='response')
m2_auc[3]=auc(temp_validation_method2$expert_label, prob)
prob=predict(logm2_4,temp_validation_method2,type='response')
m2_auc[4]=auc(temp_validation_method2$expert_label, prob)

prob=predict(logm2_5,temp_validation_method2,type='response')
m2_auc[5]=auc(temp_validation_method2$expert_label, prob)
prob=predict(logm2_6,temp_validation_method2,type='response')
m2_auc[6]=auc(temp_validation_method2$expert_label, prob)

prob=predict(logm2_7,temp_validation_method2,type='response')
m2_auc[7]=auc(temp_validation_method2$expert_label, prob)
prob=predict(logm2_8,temp_validation_method2,type='response')
m2_auc[8]=auc(temp_validation_method2$expert_label, prob)
```

```{r}
avg_auc=(m1_auc+m2_auc)/2
rounded_AUC = round(c(m1_auc,m2_auc,avg_auc),3)
tab <- matrix(rounded_AUC, ncol=3)
colnames(tab) <- c('m1_auc','m2_auc','avg_auc')
rownames(tab) <- c("NDAI","SD","CORR","DF","CF","BF","AF","AN")
tab=as.table(tab[order(tab[,3],decreasing = T),])
tab
```



# 3 Modeling

# 4 Diagnostics 


# Acknowledgement
GridExtra to combine shared labels: https://statisticsglobe.com/add-common-legend-to-combined-ggplot2-plots-in-r/
Heatmap for correlations: http://www.sthda.com/english/wiki/ggplot2-quick-correlation-matrix-heatmap-r-software-and-data-visualization

***
