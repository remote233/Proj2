---
title: "Project2"
author: "Yiliang Yuan & Yiyu Lin"
date: "2022-11-26"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
source("CVmaster.R", local = knitr::knit_global())
library(tidyverse)
library(ggplot2)
library(gridExtra)
library(GGally)
library(reshape2)
library(ROCR)
library(cvAUC)
library(dplyr)
library(pROC)
library(randomForest)
library(Metrics)
library(kableExtra)
```

***
## 1 Data Collection and Exploration
## (a)
Global climate change is an important scientific and public topic. In this topic, there’s a prediction that, in the Arctic area, surface air temperatures will have the strongest dependency on increasing atmospheric carbon dioxide levels. In further study of this dependency, the key is to detect the cloud condition. This is a challenging problem if we apply two previous MISR operational algorithms because of the similarities between the remote sensing image of clouds and that of ice- or snow-covered surface. Thus, based on the new idea of searching for cloud-free surface, the goal of this study is to develop a computationally efficient algorithm with the combination of classification and clustering methods, but without the requiring human intervention
The data in this study represents 10 MISR orbits from Path 26, which is 360 km wide, and starting at the Arctic until Antarctica. The time between each orbit is 16 days. In this path, there’re 180 blocks to collect data, but for each orbit case, only 6 blocks’ data will be taken. In these 60 blocks, the author removed 3 blocks whose cloud condition can be detected by the previous MISR algorithm. Therefore, there are 57 blocks data units with 7,114,248 1.1-km resolution pixels with 36 radiation measurements for each pixel in this study. Author hand-labeled these data, with “1” for cloudy, “-1” for not cloudy, and “0” for ambiguous.
There’re also three key features used in this study to differentiate surface pixels from cloudy pixels. The first is CORR. It is used to differentiate MISR images of the same scene from different MISR viewing directions. The second is SDAn, the standard deviation of MISR nadir camera pixel values across a scene. The last one, NDAI, is a normalized difference angular index. It can be used to characterize the changes in a scene with changes in the MISR view direction.
In conclusion, both the newly-developed ELCM and ELCM-QDA algorithm return a result with better accuracy and better coverage, when compared to the results generated from previous MISR algorithm. Moreover, the ELCM-QDA algorithm can generate more information and perform consistently on probability prediction. The impact of this work is significant. Statisticians will play a more important role in data analyses and statistical thinking will take more responsibility in solving modern scientific problems.



## (b)
```{r}
image1=read.csv('imagem1.txt',sep='',header=FALSE)
image2=read.csv('imagem2.txt', sep='',header=FALSE)
image3=read.csv('imagem3.txt', sep='',header=FALSE)
colnames(image1) = c('y','x','expert_label','NDAI','SD','CORR','DF','CF','BF','AF','AN')
colnames(image2) = c('y','x','expert_label','NDAI','SD','CORR','DF','CF','BF','AF','AN')
colnames(image3) = c('y','x','expert_label','NDAI','SD','CORR','DF','CF','BF','AF','AN')
image <- rbind(image1,image2,image3)
```

First of all, we can find the percentage of pixels for the different classes in different images as:
```{r}
per1 <- image1 %>%
  group_by(expert_label)%>%
  summarise(image1 = round(n()/nrow(.),3))
per2 <- image2 %>%
  group_by(expert_label)%>%
  summarise(image2 = round(n()/nrow(.),3))
per3 <- image3 %>%
  group_by(expert_label)%>%
  summarise(image3 = round(n()/nrow(.),3))
per_total <- image %>%
  group_by(expert_label)%>%
  summarise(image_total = round(n()/nrow(.),3))
per_combined <- list(per1,per2,per3,per_total)
per_combined <- per_combined %>% reduce(full_join, by='expert_label')
per_combined %>%
  kbl(caption="Table 1: Summary Statistics of Financial Well-Being  
               Score by Gender and Education",
       format= "latex",
                  align="r") %>%
  kable_classic(full_width = F, html_font = "helvetica") %>%
  kable_styling(latex_options = "HOLD_position")
```
From the table above, we can find that the coverage of expert label in different images are quite different. For image1, over 70% of pixels have been labeled, but for image3, only about 50% of pixels have been labeled. Moreover, the ratios of cloud and non-cloud class in each image are very different. In image1, the ratio is very close to 1:1, but for other two images, the number of pixels with non-cloud labels are obviously much more than those with cloud labels. Overall, about 40% of total pixels are not labeled. About 37% of total pixels are labeled non-cloud, while only about 23% of total pixels are labeled cloud.

Then, we can plot the maps of different images by x,y coordinates values with expert labels as colors as:
```{r}
myColors = c("gray40","black","white")
p1 = ggplot(image1, aes(x, y, colour = factor(expert_label))) + 
  geom_point(show.legend = FALSE)+
  scale_color_manual(values=myColors)+
  labs(colour="expert_label")
p2 = ggplot(image2, aes(x, y, colour = factor(expert_label))) + 
  geom_point(show.legend = FALSE)+
  scale_color_manual(values=myColors)+
  labs(colour="expert_label")
p3 = ggplot(image3, aes(x, y, colour = factor(expert_label))) + 
  geom_point(show.legend = FALSE)+
  scale_color_manual(values=myColors)+
  labs(colour="expert_label")
```
    
```{r,fig.width=8,fig.height=3, fig.cap="\\label{fig:figs}Maps"}
gg_legend <- ggplot(image1, aes(x, y, colour = factor(expert_label))) + 
  geom_point()+
  scale_color_manual(values=myColors)+
  labs(colour="expert_label") +
  theme(legend.position = "bottom")
extract_legend <- function(my_ggp) {
  step1 <- ggplot_gtable(ggplot_build(my_ggp))
  step2 <- which(sapply(step1$grobs, function(x) x$name) == "guide-box")
  step3 <- step1$grobs[[step2]]
  return(step3)
}
shared_legend <- extract_legend(gg_legend)
grid.arrange(arrangeGrob(p1,p2,p3,ncol=3), shared_legend,nrow = 2, heights = c(10, 1))
```
In the graphs above, we plot each pixels as a single dot in the graph, but they can be merged to a large area with the same color. This means pixels from each class tend to be surrounded by the pixels from the same class. Thus, for all the dataset, we cannot treat samples or pixels as i.i.d. 


## (c)
```{r,fig.width=4,fig.height=4, fig.cap="\\label{fig:figs}Heatmap for correlations between features"}
image = image %>%
  mutate(expert_label = factor(expert_label))
image_cor = round(cor(image[,4:11]),2)
get_upper_tri <- function(corr){
    corr[lower.tri(corr)]<- NA
    return(corr)
}
upper_tri = get_upper_tri(image_cor)
melted_image_corr <- melt(upper_tri, na.rm = TRUE)

heatmap <- ggplot(data = melted_image_corr, aes(Var2, Var1, fill = value))+
 geom_tile(color = "white")+
 scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
   midpoint = 0, limit = c(-1,1), space = "Lab", 
   name="Correlations") +
  theme_minimal()+ 
 theme(axis.text.x = element_text(angle = 45, vjust = 1, 
    size = 12, hjust = 1))+
 coord_fixed()

heatmap + 
geom_text(aes(Var2, Var1, label = value), color = "black", size = 4) +
theme(
  axis.title.x = element_blank(),
  axis.title.y = element_blank(),
  panel.grid.major = element_blank(),
  panel.border = element_blank(),
  panel.background = element_blank(),
  axis.ticks = element_blank(),
  legend.justification = c(1, 0),
  legend.position = c(0.6, 0.7),
  legend.direction = "horizontal")+
  guides(fill = guide_colorbar(barwidth = 7, barheight = 1,
                title.position = "top", title.hjust = 0.5))
```
\newline
Since the size of our dataset is too large and there are 7 variables to pair, it might not be a good idea to use pairs plot here. Instead, we can draw a heat map for the correlations between the features themselves, and the result is very interesting. NDAI, SD and CORR are positively correlated, and all the radiance variables are also positively correlated. However, radiance and other features(CORR,NDAI,SD) are negatively correlated. In addition, we find that radiance angle AN, AF, BF, CF are strongly correlated since their correlations are all larger than 0.75. Radiance angle DF is strongly correlated to radiance angle CF and BF, but its correlations with radiance angle AF and AN are not that strong.

```{r}
g1 = ggplot(image,aes(x=expert_label, y=NDAI))+
  geom_boxplot()
g2 = ggplot(image,aes(x=expert_label, y=SD))+
  geom_boxplot()
g3 = ggplot(image,aes(x=expert_label, y=CORR))+
  geom_boxplot()
g4 = ggplot(image,aes(x=expert_label, y=DF))+
  geom_boxplot()
g5 = ggplot(image,aes(x=expert_label, y=CF))+
  geom_boxplot()
g6 = ggplot(image,aes(x=expert_label, y=BF))+
  geom_boxplot()
g7 = ggplot(image,aes(x=expert_label, y=AF))+
  geom_boxplot()
g8 = ggplot(image,aes(x=expert_label, y=AN))+
  geom_boxplot()
```

```{r,fig.width=9,fig.height=3, fig.cap="\\label{fig:figs}Boxplots for NDAI, SD, CORR"}
grid.arrange(g1,g2,g3,ncol=3)
```
\newline
By plotting the boxplots of NDAI, SD, and CORR with expert label as x-axis, we find that cloud class tends to have higher values of NDAI, SD and CORR.
```{r,fig.width=9,fig.height=6, fig.cap="\\label{fig:figs}Boxplots for radiances with different angles"}
grid.arrange(g4,g5,g6,g7,g8,ncol=3)
```
\newline
Again, by plotting boxplots of DF, CF, BF, AF and AN with expert labels as x-axis, we find that cloud class tends to have smaller values of radiance in different angles except that in DF. For radiance angle DF, all the classes have similar distributions of radiance values, so it is hard to tell differences between cloud and no cloud classes.


# 2 Preparation

## (a)

### First Splitting method
For our first method, we want to use a simple and direct way to split our data. In order to have enough data for training and testing, we can use the data of two pictures as our training set so that the training set takes 66.6% of the entire data. Then, we only need to split the data of image3 into a validation and test set. Here, we set the ratio of the amount of data in three sets to be around 2:1:1, so validation set should take about 16.6% of the entire data and test set should take 16.6% of the entire data. We first set two images to be our training set, and split the remaining picture into two rectangles of equal size. Then, we can take one rectangle as our validation set and one as test set. In this method, we need to make sure that each rectangle we split should contain data from different classes, or be diversified. After checking the three images we plotted in 1(b), we decide to use the first two images as our training set, and split the third one. In the third image, we can set thresholds values for the y-axis, and divide the image into two rectangles according to the threshold value.
```{r}
image3_filtered = image3 %>% filter(expert_label!=0) %>% mutate(expert_label=factor(expert_label))
y_max_p3 = max(image3_filtered$y)
y_min_p3 = min(image3_filtered$y)
lim = y_max_p3-(y_max_p3-y_min_p3)/2
ggplot(image3_filtered,aes(x,y,color=expert_label))+
  geom_point()+
  scale_color_manual(values=c("grey40","blue"))+
  labs(colour="expert_label")+
  geom_hline(yintercept=lim,color="red")
```
From the plot above, we can find that our thresholds for y-axis divide the image into three rectangles containing data from different classes. We can pick the one on the top to be our test set, and the remaining two to be our validation set.
```{r}
train_method1 = rbind(image1,image2) %>% filter(expert_label!=0) %>% mutate(expert_label=factor(expert_label))
test_method1 = image3_filtered[image3_filtered$y>=lim,]
validation_method1 = image3_filtered[image3_filtered$y<lim,]
```

### Second Method
In the second method, we can use blocks to split the all the images and randomly select 60% blocks as our training set, 20% as validation set, and the remaining 20% as test set. We plan to divide each image into 100 blocks and then we will have totally 300 blocks. Then, we can divide those 300 blocks into train, validation, and test sets according to the ratio we set. For both methods, we will delete unlabeled pixels after splitting the data because they are not useful for our classification.
```{r}
set.seed(111)
image1_xcut = seq(min(image1$x),max(image1$x),length=11)
image2_xcut = seq(min(image2$x),max(image2$x),length=11)
image3_xcut = seq(min(image3$x),max(image3$x),length=11)
image1_ycut = seq(min(image1$y),max(image1$y),length=11)
image2_ycut = seq(min(image2$y),max(image2$y),length=11)
image3_ycut = seq(min(image3$y),max(image3$y),length=11)

blocks = list()
b_lab = 1
for(i in 1:10){
  for(j in 1:10){
    single_block = image1 %>% filter(x>image1_xcut[i] & x<image1_xcut[i+1] & y>image1_ycut[i] & y<image1_ycut[i+1]) %>% mutate(blocks=b_lab)
    blocks[[b_lab]] = single_block
    b_lab = b_lab+1
  }
}
for(i in 1:10){
  for(j in 1:10){
    single_block = image2 %>% filter(x>image2_xcut[i] & x<image2_xcut[i+1] & y>image2_ycut[i] & y<image2_ycut[i+1]) %>% mutate(blocks=b_lab)
    blocks[[b_lab]] = single_block
    b_lab = b_lab+1
  }
}
for(i in 1:10){
  for(j in 1:10){
    single_block = image3 %>% filter(x>image3_xcut[i] & x<image3_xcut[i+1] & y>image3_ycut[i] & y<image3_ycut[i+1]) %>% mutate(blocks=b_lab)
    blocks[[b_lab]] = single_block
    b_lab = b_lab+1
  }
}
sample_indexs = 1:300 
train_index = sample(sample_indexs,size = 180,replace = FALSE)
validation_index = sample(sample_indexs[-train_index],size=60,replace = FALSE)
test_index = sample_indexs[-c(train_index,validation_index)]

train_method2 = blocks[[train_index[1]]]
validation_method2 = blocks[[validation_index[1]]]
test_method2 = blocks[[test_index[1]]]
for(i in train_index[-1]){
  train_method2 = rbind(train_method2,blocks[[i]])
}
for(i in validation_index[-1]){
  validation_method2 = rbind(validation_method2,blocks[[i]])
}
for(i in test_index[-1]){
  test_method2 = rbind(test_method2,blocks[[i]])
}
train_method2 = train_method2%>%filter(expert_label!=0)%>%mutate(expert_label=factor(expert_label))
test_method2 = test_method2%>%filter(expert_label!=0)%>%mutate(expert_label=factor(expert_label))
validation_method2 = validation_method2%>%filter(expert_label!=0)%>%mutate(expert_label=factor(expert_label))
```


## (b)
The accuracy of the trivial classifier on the validation and test set of two methods is:
```{r}
# accuracy of method1
accuracy = matrix(NA,nrow=2,ncol=2,dimnames=list(c("validation","test"),c("method1","method2")))
accuracy[1,1] = sum(validation_method1$expert_label==-1)/nrow(validation_method1)
accuracy[1,2] = sum(validation_method2$expert_label==-1)/nrow(validation_method2)
accuracy[2,1] = sum(test_method1$expert_label==-1)/nrow(test_method1)
accuracy[2,2] = sum(test_method2$expert_label==-1)/nrow(test_method2)
accuracy
```
This kind of trivial classifier will have high accuracy when a dataset consist almost entirely of cloud-free class. On the table above, we can find that the trivial classifier does not perform well on most of the sets. For the validation set of our method1, since cloud-free class occupies a large proportion of the data, the accuracy of trivial classifier is pretty high compared with other sets, but the accuracy is still only around 0.75. Therefore, we would say our classification problem is not trivial.

## (c)
In order to determine 3 best features out of 8 features, we firstly used random forest to find and plot the importance of each feature based on train sets of both methods. We found that NDAI, SD and CORR were the features having the greatest importance in both plots. Then, we decided to use each feature to build a single-predictor logistics regression model based on training data set from both methods, and use the validation data sets from both methods to draw the ROC curve.
We got two area under the ROC curve (AUC) values, and made them equal weighted to get the average AUC value to evaluate the overall performance of each model. 
An ideal ROC curve will have huge top left corner, so the larger the average AUC value the better the classifier. Again, NDAI, SD and CORR had the largest average AUC values. So, combined with the result of random forest, we decided to use NDAI, SD, and CORR as our three "best" features. Among them, NDAI performed the best in both random forest and ROC curves, so it should be the most useful variable for our classification.

```{r}
# change expert_label's not cloudy label from -1 to 0
temp_train_method1=train_method1 %>% 
   mutate(expert_label = ifelse(expert_label == -1,0,1))

temp_train_method2=train_method2 %>%
  mutate(expert_label = ifelse(expert_label == -1,0,1))

temp_validation_method1=validation_method1 %>%
  mutate(expert_label = ifelse(expert_label == -1,0,1))

temp_validation_method2=validation_method2 %>%
  mutate(expert_label = ifelse(expert_label == -1,0,1))
```

```{r}
cloud_RF1 = randomForest(as.factor(expert_label)~NDAI+SD+CORR+AF+BF+CF+DF+AN,data=temp_train_method1,ntree=100)
cloud_RF2 = randomForest(as.factor(expert_label)~NDAI+SD+CORR+AF+BF+CF+DF+AN,data=temp_train_method2,ntree=100)
varImpPlot(cloud_RF1)
varImpPlot(cloud_RF2)
```

```{r}
logm1_1=glm(expert_label~NDAI,data=temp_train_method1,family = "binomial")
logm1_2=glm(expert_label~SD,data=temp_train_method1,family = "binomial")
logm1_3=glm(expert_label~CORR,data=temp_train_method1,family = "binomial")
logm1_4=glm(expert_label~DF,data=temp_train_method1,family = "binomial")
logm1_5=glm(expert_label~CF,data=temp_train_method1,family = "binomial")
logm1_6=glm(expert_label~BF,data=temp_train_method1,family = "binomial")
logm1_7=glm(expert_label~AF,data=temp_train_method1,family = "binomial")
logm1_8=glm(expert_label~AN,data=temp_train_method1,family = "binomial")
```
```{r}
m1_auc=c()
prob=predict(logm1_1,temp_validation_method1,type='response')
m1_auc[1]=auc(temp_validation_method1$expert_label, prob)
prob=predict(logm1_2,temp_validation_method1,type='response')
m1_auc[2]=auc(temp_validation_method1$expert_label, prob)

prob=predict(logm1_3,temp_validation_method1,type='response')
m1_auc[3]=auc(temp_validation_method1$expert_label, prob)
prob=predict(logm1_4,temp_validation_method1,type='response')
m1_auc[4]=auc(temp_validation_method1$expert_label, prob)

prob=predict(logm1_5,temp_validation_method1,type='response')
m1_auc[5]=auc(temp_validation_method1$expert_label, prob)
prob=predict(logm1_6,temp_validation_method1,type='response')
m1_auc[6]=auc(temp_validation_method1$expert_label, prob)

prob=predict(logm1_7,temp_validation_method1,type='response')
m1_auc[7]=auc(temp_validation_method1$expert_label, prob)
prob=predict(logm1_8,temp_validation_method1,type='response')
m1_auc[8]=auc(temp_validation_method1$expert_label, prob)
```

```{r}
logm2_1=glm(expert_label~NDAI,data=temp_train_method2,family = "binomial")
logm2_2=glm(expert_label~SD,data=temp_train_method2,family = "binomial")
logm2_3=glm(expert_label~CORR,data=temp_train_method2,family = "binomial")
logm2_4=glm(expert_label~DF,data=temp_train_method2,family = "binomial")
logm2_5=glm(expert_label~CF,data=temp_train_method2,family = "binomial")
logm2_6=glm(expert_label~BF,data=temp_train_method2,family = "binomial")
logm2_7=glm(expert_label~AF,data=temp_train_method2,family = "binomial")
logm2_8=glm(expert_label~AN,data=temp_train_method2,family = "binomial")
```
```{r}
library(ROCR)
m2_auc=c()
prob=predict(logm2_1,temp_validation_method2,type='response')
m2_auc[1]=auc(temp_validation_method2$expert_label, prob)
prob=predict(logm2_2,temp_validation_method2,type='response')
m2_auc[2]=auc(temp_validation_method2$expert_label, prob)

prob=predict(logm2_3,temp_validation_method2,type='response')
m2_auc[3]=auc(temp_validation_method2$expert_label, prob)
prob=predict(logm2_4,temp_validation_method2,type='response')
m2_auc[4]=auc(temp_validation_method2$expert_label, prob)

prob=predict(logm2_5,temp_validation_method2,type='response')
m2_auc[5]=auc(temp_validation_method2$expert_label, prob)
prob=predict(logm2_6,temp_validation_method2,type='response')
m2_auc[6]=auc(temp_validation_method2$expert_label, prob)

prob=predict(logm2_7,temp_validation_method2,type='response')
m2_auc[7]=auc(temp_validation_method2$expert_label, prob)
prob=predict(logm2_8,temp_validation_method2,type='response')
m2_auc[8]=auc(temp_validation_method2$expert_label, prob)
```

```{r}
avg_auc=(m1_auc+m2_auc)/2
rounded_AUC = round(c(m1_auc,m2_auc,avg_auc),3)
tab <- matrix(rounded_AUC, ncol=3)
colnames(tab) <- c('m1_auc','m2_auc','avg_auc')
rownames(tab) <- c("NDAI","SD","CORR","DF","CF","BF","AF","AN")
tab=as.table(tab[order(tab[,3],decreasing = T),])
tab
```

# 3 Modeling

## (a)

### How to create folds.
Firstly, we need to divide our datasets into k-folds. Let assume we would like to divide 10 folds. Due to the spatial dependency, we cannot randomly assign each pixel to a fold. Therefore, for the dataset we got by method1, we could divide the combination of training and validation data into 10 blocks by their x-axis values as below. We would combine the data of each column and make it as a fold. Then, by this method, we got 10 folds for our training and validation data from method1.
```{r,fig.width=8,fig.height=3}
cut1 = seq(min(image1$x),max(image1$x),length=11)[-c(1,11)]
cut2 = seq(min(image2$x),max(image2$x),length=11)[-c(1,11)]
cut3 = seq(min(image3$x),max(image3$x),length=11)[-c(1,11)]
cut_p1 = image1 %>% 
  filter(expert_label!=0) %>%
  mutate(expert_label=as.factor(expert_label))%>%
  ggplot(.,aes(x,y,color=expert_label))+
  geom_point(show.legend = FALSE)+
  geom_vline(xintercept = cut1, color="red")
cut_p2 = image2 %>% 
  filter(expert_label!=0) %>%
  mutate(expert_label=as.factor(expert_label))%>%
  ggplot(.,aes(x,y,color=expert_label))+
  geom_point(show.legend = FALSE)+
  geom_vline(xintercept = cut2, color="red")
cut_p3 = ggplot(validation_method1,aes(x,y,color=expert_label))+
  geom_point(show.legend = FALSE)+
  geom_vline(xintercept = cut2, color="red")
gg_legend <- ggplot(validation_method1,aes(x,y,color=expert_label)) + 
  geom_point()+
  theme(legend.position = "bottom")
shared_legend <- extract_legend(gg_legend)
grid.arrange(arrangeGrob(cut_p1,cut_p2,cut_p3,ncol=3), shared_legend,nrow = 2, heights = c(10, 1))
```
Then, for the training and validation data from method2, we could extract the index of the blocks in our data first. For some blocks, since they do not contain any labeled pixel, they might already be deleted, so we may not have 270 blocks as we expected.After we . Therefore, we could get 10 folds as well. In order to make it more convenient for us to calculate, we should divide the fold for our training data before we use the CVmaster and give a fold number to each pixel in the training data.

### CVV accuracy

```{r}
#split folds for data from method1
image1_split = image1 %>% mutate(folds = as.integer((x-min(x)-1)/(max(x)-min(x))*10)+1)
image2_split = image2 %>% mutate(folds = as.integer((x-min(x)-1)/(max(x)-min(x))*10)+1)
image3_split = validation_method1 %>% mutate(folds = as.integer((x-min(x)-1)/(max(x)-min(x))*10)+1)
train_cv1 = rbind(image1_split,image2_split,image3_split)
train_cv1 = train_cv1 %>% 
  filter(expert_label!=0)%>%
  mutate(expert_label = ifelse(expert_label==-1,0,1)) %>%
  mutate_at(c("NDAI", "SD", "CORR"), ~(scale(.) %>% as.vector))
# scale features and select inputs
train_feature1 = train_cv1 %>% dplyr::select(NDAI,SD,CORR)
train_label1 = train_cv1 %>% dplyr::select(expert_label)
train_folds1 = train_cv1 %>% dplyr::select(folds)
```

```{r}
#split folds for data from method2
train_cv2 = rbind(train_method2,validation_method2)
train_cv2$new_blocks <- match(train_cv2$blocks, unique(train_cv2$blocks))
# number of folds
K=10
train_cv2 = train_cv2 %>% 
  mutate(folds = new_blocks%%K+1, expert_label = ifelse(expert_label==-1,0,1)) %>% 
  mutate_at(c("NDAI", "SD", "CORR"), ~(scale(.) %>% as.vector))
# scale the features
train_feature2 = train_cv2 %>% dplyr::select(NDAI,SD,CORR)
train_label2 = train_cv2 %>% dplyr::select(expert_label)
train_folds2 = train_cv2 %>% dplyr::select(folds)
```

```{r}
#try different models here
source("CVmaster.R", local = knitr::knit_global())
lr_result1 <- CVmaster(lr_model, train_feature1, train_label1, train_folds1)
nb_result1 <- CVmaster(nb_model, train_feature1, train_label1, train_folds1)
lda_result1 <- CVmaster(lda_model, train_feature1, train_label1, train_folds1)
qda_result1 <- CVmaster(qda_model, train_feature1, train_label1, train_folds1)
knn_result1 <- CVmaster(knn_model, train_feature1, train_label1, train_folds1)
xgb_result1 <- CVmaster(XGBoost_model, train_feature1, train_label1, train_folds1)

lr_result2 <- CVmaster(lr_model, train_feature2, train_label2, train_folds2)
nb_result2 <- CVmaster(nb_model, train_feature2, train_label2, train_folds2)
lda_result2 <- CVmaster(lda_model, train_feature2, train_label2, train_folds2)
qda_result2 <- CVmaster(qda_model, train_feature2, train_label2, train_folds2)
knn_result2 <- CVmaster(knn_model, train_feature2, train_label2, train_folds2)
xgb_result2 <- CVmaster(XGBoost_model, train_feature2, train_label2, train_folds2)
```

```{r}
# accuracy table for both datasets with different models
result_list1 <- list(lr_result1, nb_result1, lda_result1, qda_result1, knn_result1, xgb_result1)
result_table1 <- result_list1 %>% reduce(full_join, by='folds')
print(result_table1)

result_list2 <- list(lr_result2, nb_result2, lda_result2, qda_result2, knn_result2, xgb_result2)
result_table2 <- result_list2 %>% reduce(full_join, by="folds")
print(result_table2)
```

```{r}
# test accuracy
test1 <- test_method1 %>% 
  mutate(expert_label=ifelse(expert_label==-1,0,1)) %>%
  mutate_at(c("NDAI", "SD", "CORR"), ~(scale(.) %>% as.vector))
test2 <- test_method2 %>% 
  mutate(expert_label=ifelse(expert_label==-1,0,1)) %>%
  mutate_at(c("NDAI", "SD", "CORR"), ~(scale(.) %>% as.vector))
train1 <- rbind(train_method1,validation_method1) %>%
  mutate(expert_label=ifelse(expert_label==-1,0,1)) %>%
  mutate_at(c("NDAI", "SD", "CORR"), ~(scale(.) %>% as.vector))
train2 <- rbind(train_method2,validation_method2) %>% 
  mutate(expert_label=ifelse(expert_label==-1,0,1)) %>%
  mutate_at(c("NDAI", "SD", "CORR"), ~(scale(.) %>% as.vector))
test_accuracy <- matrix(NA,ncol=2,nrow=6)
rownames(test_accuracy) <- c("Logistic Regression", "Naive Bayes", "QDA", "LDA", "KNN", "XGBoost")
colnames(test_accuracy) <- c("split method1", "split method2")
form <- formula("expert_label ~ NDAI+SD+CORR")

# Logistic Regression
lr_mod1 <- glm(form, data=train1,family = "binomial")
lr_pred1 <- predict(lr_mod1, test1, type="response")
pred = rep(0,length(lr_pred1))
pred[lr_pred1>0.5] = 1
test_accuracy[1,1] <- round(mean(pred==test1$expert_label),3)

lr_mod2 <- glm(form, data=train2,family = "binomial")
lr_pred2 <- predict(lr_mod2, test2, type="response")
pred = rep(0,length(lr_pred2))
pred[lr_pred2>0.5] = 1
test_accuracy[1,2] <- round(mean(pred==test2$expert_label),3)

# Naive Bayes
nb_mod1 <- naiveBayes(form, data=train1)
nb_pred1 = predict(nb_mod1, test1)
test_accuracy[2,1] = round(mean(nb_pred1==test1$expert_label),3)
nb_mod2 <- naiveBayes(form, data=train2)
nb_pred2 = predict(nb_mod2, test2)
test_accuracy[2,2] = round(mean(nb_pred2==test2$expert_label),3)

# QDA
qda_mod1 <- qda(form, data=train1)
qda_pred1 = predict(qda_mod1, test1)
test_accuracy[3,1] = round(mean(qda_pred1$class==test1$expert_label),3)

qda_mod2 <- qda(form, data=train2)
qda_pred2 = predict(qda_mod2, test2)
test_accuracy[3,2] = round(mean(qda_pred2$class==test2$expert_label),3)

# LDA
lda_mod1 <- lda(form, data=train1)
lda_pred1 = predict(lda_mod1, test1)
test_accuracy[4,1] = round(mean(lda_pred1$class==test1$expert_label),3)
lda_mod2 <- lda(form, data=train2)
lda_pred2 = predict(lda_mod2, test2)
test_accuracy[4,2] = round(mean(lda_pred2$class==test2$expert_label),3)

# KNN
knn_mod1 <- knn(train1[,c("NDAI","SD","CORR")], test1[,c("NDAI","SD","CORR")], train1$expert_label, k=20)
test_accuracy[5,1] <- round(mean(knn_mod1==test1$expert_label),3)
knn_mod2 <- knn(train2[,c("NDAI","SD","CORR")], test2[,c("NDAI","SD","CORR")], train2$expert_label, k=20)
test_accuracy[5,2] <- round(mean(knn_mod2==test2$expert_label),3)

# XGBoost
xgb_train = xgb.DMatrix(data = data.matrix(train1[,c("NDAI","SD","CORR")]), label = train1$expert_label)
xgb_test = xgb.DMatrix(data = data.matrix(test1[,c("NDAI","SD","CORR")]), label = test1$expert_label)
watchlist = list(train=xgb_train, test=xgb_test)
model = xgb.train(data = xgb_train, max.depth = 3, watchlist=watchlist, nrounds = 1000, verbose = 0)
final_rounds = as.integer(model$evaluation_log[which.min(test_rmse),1])
train_xgb1 = xgboost(data = xgb_train, max.depth = 3, nrounds = final_rounds, verbose = 0)
xgb_pred1 = predict(train_xgb1, xgb_test)
pred = rep(0,length(xgb_pred1))
pred[xgb_pred1>0.5] = 1
test_accuracy[6,1] <- round(mean(pred==test1$expert_label),3)

xgb_train2 = xgb.DMatrix(data = data.matrix(train2[,c("NDAI","SD","CORR")]), label = train2$expert_label)
xgb_test2 = xgb.DMatrix(data = data.matrix(test2[,c("NDAI","SD","CORR")]), label = test2$expert_label)
watchlist = list(train=xgb_train2, test=xgb_test2)
model = xgb.train(data = xgb_train2, max.depth = 3, watchlist=watchlist, nrounds = 1000, verbose = 0)
final_rounds = as.integer(model$evaluation_log[which.min(test_rmse),1])
train_xgb2 = xgboost(data = xgb_train2, max.depth = 3, nrounds = final_rounds, verbose = 0)
xgb_pred2 = predict(train_xgb2, xgb_test2)
pred = rep(0,length(xgb_pred2))
pred[xgb_pred2>0.5] = 1
test_accuracy[6,2] <- round(mean(pred==test2$expert_label),3)

test_accuracy
```
## 3(b)
```{r}
euclidean=function(a, b){
  sqrt(sum((a - b)^2))
  }

library(e1071)
library(ROCR)
# Logistis regression
probs_lr <- predict(lr_mod2, test2, type="response")
pred_lr <- prediction(probs_lr, test2$expert_label)
perf_lr <- performance(pred_lr, measure='tpr', x.measure='fpr')
roc_lr <- data.frame(fpr=unlist(perf_lr@x.values), tpr=unlist(perf_lr@y.values))
roc_lr$method <- "logistic regression"

min_distance=100
lr_record_x=0
lr_record_y=0
for(i in 1:length(roc_lr$fpr)){
  current_distance=euclidean(c(roc_lr$fpr[i],roc_lr$tpr[i]),c(0,1))
  if (current_distance<min_distance){
    min_distance=current_distance
    lr_record_x=roc_lr$fpr[i]
    lr_record_y=roc_lr$tpr[i]
  }
}

p1=ggplot(data=rbind(roc_lr),aes(x=fpr, y=tpr, linetype=method)) + 
  geom_line() +
  geom_abline(intercept =0, slope = 1, linetype=2) +
  geom_point(aes(x=lr_record_x,y=lr_record_y),col="red")+
  ggtitle("roc curve of logistics regression")

# Naive Bayes
probs_nb <- predict(nb_mod2, test2, type="raw")
pred_nb <- prediction(probs_nb[, 2], test2$expert_label)
perf_nb <- performance(pred_nb, measure='tpr', x.measure='fpr')
roc_nb <- data.frame(fpr=unlist(perf_nb@x.values), tpr=unlist(perf_nb@y.values))
roc_nb$method <- "naive bayes"

min_distance=100
nb_record_x=0
nb_record_y=0
for(i in 1:length(roc_nb$fpr)){
  current_distance=euclidean(c(roc_nb$fpr[i],roc_nb$tpr[i]),c(0,1))
  if (current_distance<min_distance){
    min_distance=current_distance
    nb_record_x=roc_nb$fpr[i]
    nb_record_y=roc_nb$tpr[i]
  }
}

p2=ggplot(data=rbind(roc_nb),aes(x=fpr, y=tpr, linetype=method)) + 
  geom_line() +
  geom_abline(intercept =0, slope = 1, linetype=2) +
  geom_point(aes(x=nb_record_x,y=nb_record_y),col="red")+
  ggtitle("roc curve of naive bayes")

# QDA
probs_qda = predict(qda_mod2, test2)
pred_qda <- prediction(probs_qda$posterior[,2], test2$expert_label)
perf_qda <- performance(pred_qda, measure='tpr', x.measure='fpr')
roc_qda <- data.frame(fpr=unlist(perf_qda@x.values), tpr=unlist(perf_qda@y.values))
roc_qda$method <- "QDA"

min_distance=100
qda_record_x=0
qda_record_y=0
for(i in 1:length(roc_qda$fpr)){
  current_distance=euclidean(c(roc_qda$fpr[i],roc_qda$tpr[i]),c(0,1))
  if (current_distance<min_distance){
    min_distance=current_distance
    qda_record_x=roc_qda$fpr[i]
    qda_record_y=roc_qda$tpr[i]
  }
}
p3=ggplot(data=rbind(roc_qda),aes(x=fpr, y=tpr, linetype=method)) + 
  geom_line() +
  geom_abline(intercept =0, slope = 1, linetype=2) +
  geom_point(aes(x=qda_record_x,y=qda_record_y),col="red")+
  ggtitle("roc curve of qda")

# LDA
probs_lda = predict(lda_mod2, test2)
pred_lda <- prediction(probs_lda$posterior[,2], test2$expert_label)
perf_lda <- performance(pred_lda, measure='tpr', x.measure='fpr')
roc_lda <- data.frame(fpr=unlist(perf_lda@x.values), tpr=unlist(perf_lda@y.values))
roc_lda$method <- "LDA"

min_distance=100
lda_record_x=0
lda_record_y=0
for(i in 1:length(roc_lda$fpr)){
  current_distance=euclidean(c(roc_lda$fpr[i],roc_lda$tpr[i]),c(0,1))
  if (current_distance<min_distance){
    min_distance=current_distance
    lda_record_x=roc_lda$fpr[i]
    lda_record_y=roc_lda$tpr[i]
  }
}

p4=ggplot(data=rbind(roc_lda),aes(x=fpr, y=tpr, linetype=method)) + 
  geom_line() +
  geom_abline(intercept =0, slope = 1, linetype=2) +
  geom_point(aes(x=lda_record_x,y=lda_record_y),col="red")+
  ggtitle("roc curve of lda")


# KNN
#knn_mod2 <- knn(train2[,c("NDAI","SD","CORR")], test2[,c("NDAI","SD","CORR")], train2$expert_label, k=20)
knn_mod2 <- class::knn(train2[,c("NDAI","SD","CORR")],test2[,c("NDAI","SD","CORR")], train2$expert_label, k=20, prob=TRUE)
probs_knn <- attr(knn_mod2, "prob")
probs_knn <- 2*ifelse(knn_mod2 == "0", 1-probs_knn, probs_knn) - 1
pred_knn <- prediction(probs_knn, test2$expert_label)
perf_knn=performance(pred_knn, measure = "tpr",x.measure =  "fpr")
roc_knn <- data.frame(fpr=unlist(perf_knn@x.values), tpr=unlist(perf_knn@y.values))
roc_knn$method <- "KNN"

min_distance=100
knn_record_x=0
knn_record_y=0
for(i in 1:length(roc_knn$fpr)){
  current_distance=euclidean(c(roc_knn$fpr[i],roc_knn$tpr[i]),c(0,1))
  if (current_distance<min_distance){
    min_distance=current_distance
    knn_record_x=roc_knn$fpr[i]
    knn_record_y=roc_knn$tpr[i]
  }
}
p5=ggplot(data=rbind(roc_knn),aes(x=fpr, y=tpr, linetype=method)) + 
  geom_line() +
  geom_abline(intercept =0, slope = 1, linetype=2) +
  geom_point(aes(x=knn_record_x,y=knn_record_y),col="red")+
  ggtitle("roc curve of knn")


# XGBoost
probs_XGBoost = predict(train_xgb2, xgb_test2,type="response")
pred_XGBoost <- prediction(probs_XGBoost, test2$expert_label)
perf_XGBoost <- performance(pred_XGBoost, measure='tpr', x.measure='fpr')
roc_XGBoost <- data.frame(fpr=unlist(perf_XGBoost@x.values), tpr=unlist(perf_XGBoost@y.values))
roc_XGBoost$method <- "XGBoost"

min_distance=100
XGBoost_record_x=0
XGBoost_record_y=0
for(i in 1:length(roc_XGBoost$fpr)){
  current_distance=euclidean(c(roc_XGBoost$fpr[i],roc_XGBoost$tpr[i]),c(0,1))
  if (current_distance<min_distance){
    min_distance=current_distance
    XGBoost_record_x=roc_XGBoost$fpr[i]
    XGBoost_record_y=roc_XGBoost$tpr[i]
  }
}
p6=ggplot(data=rbind(roc_XGBoost),aes(x=fpr, y=tpr, linetype=method)) + 
  geom_line() +
  geom_abline(intercept =0, slope = 1, linetype=2) +
  geom_point(aes(x=XGBoost_record_x,y=XGBoost_record_y),col="red")+
  ggtitle("roc curve of XGBoost")

grid.arrange(p1,p2,p3,p4,p5,p6,nrow=3)


#ggplot(data=rbind(roc_lr,roc_nb,roc_qda,roc_lda,roc_knn,roc_XGBoost),aes(x=fpr, y=tpr, linetype=method, color=method)) + 
#  geom_line() +
#  geom_abline(intercept =0, slope = 1, linetype=2) +
#  theme(legend.position=c(0.8,0.2), legend.title=element_blank())
```
The cutoff point is highlighted as a red point in each ROC curve plot.
In this project, we need to consider the cost of increasing true positive rate (fraction of cloudy sample that are classified as cloudy) and the cost of decreasing false positive rate (fraction of not cloudy sample that are classified as cloudy). This is the same as we need to control the cost of decreasing false negative rate (fraction of not cloudy sample that are classified as cloudy), and control the cost of decreasing false positive rate. In this project, because both classification error will lead to incorrectly temperature prediction, we assume the cost of decrease these two error are the same. Therefore, we decide the cutoff point is the point whose Euclidean distance from top left corner in ROC curve plot is minimum. The cutoff point is highlighted implies the probability threshold in classification. 

## (c)
In order to assess the fit of our model more accurately, we decided to find the F1 score of our models.
F1 score is a metric which is calculated as: $$F1 = 2 * \frac{Precision*Recall}{Precision+Recall}$$. A high F1 score means the false positives and false negatives of our model is pretty low. Therefore, the higher the F1 score, the better our model is.
```{r}
F1_table <- matrix(NA, ncol=2, nrow=6)
colnames(F1_table) <- c("split method1", "split method2")
rownames(F1_table) <- c("Logistic Regression", "Naive Bayes", "QDA", "LDA", "KNN", "XGBoost")

#logistic regression
pred = rep(0, length(test1$expert_label))
pred[lr_pred1>0.5] = 1
F1_table[1,1] <- confusionMatrix(as.factor(pred), as.factor(test1$expert_label))$byClass["F1"]
pred = rep(0, length(test2$expert_label))
pred[lr_pred2>0.5] = 1
F1_table[1,2] <- confusionMatrix(as.factor(pred), as.factor(test2$expert_label))$byClass["F1"]

#naive bayes
F1_table[2,1] <- confusionMatrix(as.factor(nb_pred1), as.factor(test1$expert_label))$byClass["F1"]
F1_table[2,2] <- confusionMatrix(as.factor(nb_pred2), as.factor(test2$expert_label))$byClass["F1"]

#QDA
F1_table[3,1] <- confusionMatrix(as.factor(qda_pred1$class), as.factor(test1$expert_label))$byClass["F1"]
F1_table[3,2] <- confusionMatrix(as.factor(qda_pred2$class), as.factor(test2$expert_label))$byClass["F1"]

#LDA
F1_table[4,1] <- confusionMatrix(as.factor(lda_pred1$class), as.factor(test1$expert_label))$byClass["F1"]
F1_table[4,2] <- confusionMatrix(as.factor(lda_pred2$class), as.factor(test2$expert_label))$byClass["F1"]

#KNN
F1_table[5,1] <- confusionMatrix(as.factor(knn_mod1), as.factor(test1$expert_label))$byClass["F1"]
F1_table[5,2] <- confusionMatrix(as.factor(knn_mod2), as.factor(test2$expert_label))$byClass["F1"]

#XGBoost
pred = rep(0,length(xgb_pred1))
pred[xgb_pred1>0.5] = 1
F1_table[6,1] <- confusionMatrix(as.factor(pred), as.factor(test1$expert_label))$byClass["F1"]
pred = rep(0,length(xgb_pred2))
pred[xgb_pred2>0.5] = 1
F1_table[6,2] <- confusionMatrix(as.factor(pred), as.factor(test2$expert_label))$byClass["F1"]

print(round(F1_table,3))
```

From the F1 score table above, we can see that KNN and XGBoost models have the highest F1 scores in both dataset, so they are still the best two models among all of the models we tried.

# 4 Diagnostics 
## (a)


# Acknowledgement
GridExtra to combine shared labels: https://statisticsglobe.com/add-common-legend-to-combined-ggplot2-plots-in-r/
Heatmap for correlations: http://www.sthda.com/english/wiki/ggplot2-quick-correlation-matrix-heatmap-r-software-and-data-visualization

***
