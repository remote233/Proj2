---
title: "Project2"
author: "Yiliang Yuan & Yiyu Lin"
date: "2022-11-26"
output: word_document
header-includes: |
  \usepackage{titlesec}
  \titlespacing{\section}{0pt}{12pt plus 2pt minus 1pt}{0pt plus 1pt minus 1pt}
  \titlespacing{\subsection}{0pt}{12pt plus 2pt minus 1pt}{0pt plus 1pt minus 1pt}
  \titlespacing{\subsubsection}{0pt}{12pt plus 2pt minus 1pt}{0pt plus 1pt minus 1pt}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE )
source("CVmaster.R", local = knitr::knit_global())
library(tidyverse)
library(ggplot2)
library(gridExtra)
library(GGally)
library(reshape2)
library(ROCR)
library(cvAUC)
library(dplyr)
library(pROC)
library(randomForest)
library(Metrics)
library(kableExtra)
library(knitr)
```

***
## 1 Data Collection and Exploration

## (a)
Global climate change is an important scientific and public topic. In this topic, there’s a prediction that, in the Arctic area, surface air temperatures will have the strongest dependency on increasing atmospheric carbon dioxide levels. In further study of this dependency, the key is to detect the cloud condition. This is a challenging problem if we apply two previous MISR operational algorithms because of the similarities between the remote sensing image of clouds and that of ice- or snow-covered surface. Thus, based on the new idea of searching for cloud-free surface, the goal of this study is to develop a computationally efficient algorithm with the combination of classification and clustering methods, but without the requiring human intervention
The data in this study represents 10 MISR orbits from Path 26, which is 360 km wide, and starting at the Arctic until Antarctica. The time between each orbit is 16 days. In this path, there’re 180 blocks to collect data, but for each orbit case, only 6 blocks’ data will be taken. In these 60 blocks, the author removed 3 blocks whose cloud condition can be detected by the previous MISR algorithm. Therefore, there are 57 blocks data units with 7,114,248 1.1-km resolution pixels with 36 radiation measurements for each pixel in this study. Author hand-labeled these data, with “1” for cloudy, “-1” for not cloudy, and “0” for ambiguous.
There’re also three key features used in this study to differentiate surface pixels from cloudy pixels. The first is CORR. It is used to differentiate MISR images of the same scene from different MISR viewing directions. The second is SDAn, the standard deviation of MISR nadir camera pixel values across a scene. The last one, NDAI, is a normalized difference angular index. It can be used to characterize the changes in a scene with changes in the MISR view direction.
In conclusion, both the newly-developed ELCM and ELCM-QDA algorithm return a result with better accuracy and better coverage, when compared to the results generated from previous MISR algorithm. Moreover, the ELCM-QDA algorithm can generate more information and perform consistently on probability prediction. The impact of this work is significant. Statisticians will play a more important role in data analyses and statistical thinking will take more responsibility in solving modern scientific problems.



## (b)
```{r}
image1=read.csv('imagem1.txt',sep='',header=FALSE)
image2=read.csv('imagem2.txt', sep='',header=FALSE)
image3=read.csv('imagem3.txt', sep='',header=FALSE)
colnames(image1) = c('y','x','expert_label','NDAI','SD','CORR','DF','CF','BF','AF','AN')
colnames(image2) = c('y','x','expert_label','NDAI','SD','CORR','DF','CF','BF','AF','AN')
colnames(image3) = c('y','x','expert_label','NDAI','SD','CORR','DF','CF','BF','AF','AN')
image1$image_num = 1
image2$image_num = 2
image3$image_num = 3
image <- rbind(image1,image2,image3)
```

First of all, we can find the percentage of pixels for the different classes in different images as:
```{r}
per1 <- image1 %>%
  group_by(expert_label)%>%
  summarise(image1 = round(n()/nrow(.),3))
per2 <- image2 %>%
  group_by(expert_label)%>%
  summarise(image2 = round(n()/nrow(.),3))
per3 <- image3 %>%
  group_by(expert_label)%>%
  summarise(image3 = round(n()/nrow(.),3))
per_total <- image %>%
  group_by(expert_label)%>%
  summarise(image_total = round(n()/nrow(.),3))
per_combined <- list(per1,per2,per3,per_total)
per_combined <- per_combined %>% reduce(full_join, by='expert_label')
per_combined
```

From the Table 1 above, we can find that the coverage of expert label in different images are quite different. For image1, over 70% of pixels have been labeled, but for image3, only about 50% of pixels have been labeled. Moreover, the ratios of cloud and non-cloud class in each image are very different. In image1, the ratio is very close to 1:1, but for other two images, the number of pixels with non-cloud labels are obviously much more than those with cloud labels. Overall, about 40% of total pixels are not labeled. About 37% of total pixels are labeled non-cloud, while only about 23% of total pixels are labeled cloud.

Then, we can plot the maps of different images by x,y coordinates values with expert labels as colors as:
```{r}
myColors = c("gray40","black","white")
p1 = ggplot(image1, aes(x, y, colour = factor(expert_label))) + 
  geom_point(show.legend = FALSE)+
  scale_color_manual(values=myColors)+
  labs(colour="expert_label")
p2 = ggplot(image2, aes(x, y, colour = factor(expert_label))) + 
  geom_point(show.legend = FALSE)+
  scale_color_manual(values=myColors)+
  labs(colour="expert_label")
p3 = ggplot(image3, aes(x, y, colour = factor(expert_label))) + 
  geom_point(show.legend = FALSE)+
  scale_color_manual(values=myColors)+
  labs(colour="expert_label")
```
    
```{r,fig.width=8,fig.height=3, fig.cap="\\label{fig:figs}Maps"}
gg_legend <- ggplot(image1, aes(x, y, colour = factor(expert_label))) + 
  geom_point()+
  scale_color_manual(values=myColors)+
  labs(colour="expert_label") +
  theme(legend.position = "bottom")
extract_legend <- function(my_ggp) {
  step1 <- ggplot_gtable(ggplot_build(my_ggp))
  step2 <- which(sapply(step1$grobs, function(x) x$name) == "guide-box")
  step3 <- step1$grobs[[step2]]
  return(step3)
}
shared_legend <- extract_legend(gg_legend)
grid.arrange(arrangeGrob(p1,p2,p3,ncol=3), shared_legend,nrow = 2, heights = c(10, 1))
```

In the graphs above, we plot each pixels as a single dot in the graph, but they can be merged to a large area with the same color. This means pixels from each class tend to be surrounded by the pixels from the same class. Thus, for all the dataset, we cannot treat samples or pixels as i.i.d. 


## (c)
Since the size of our dataset is too large and there are 7 variables to pair, it might not be a good idea to use pairs plot here. Instead, we can draw a heat map for the correlations between the features themselves, and the result is very interesting. NDAI, SD and CORR are positively correlated, and all the radiance variables are also positively correlated. However, radiance and other features(CORR,NDAI,SD) are negatively correlated. In addition, we find that radiance angle AN, AF, BF, CF are strongly correlated since their correlations are all larger than 0.75. Radiance angle DF is strongly correlated to radiance angle CF and BF, but its correlations with radiance angle AF and AN are not that strong.

After findin the correlations between features, we also want to explore the relationship between the expert labels and each feature. We decided to draw boxplots of each feature with different labels. Therefore, for each feature, we will have three boxplots. By viewing the boxplots of a feature, we can see whether its distributions are different with different expert labels. If the distributions of a feature are different with different expert labels, we would say that feature might be useful in predicting the label of pixels. In figure 3, we find that the distributions of NDAI, SD AND CORR are obviously different with different expert labels, and cloud class tends to have higher values of NDAI, SD and CORR. Also, in figure 4, we find that the distributions of radiance in different angles are different with different expert labels excpet the radiance in DF. Cloud class tends to have smaller values of radiance in different angles except that in DF. For radiance angle DF, all the classes have similar distributions of radiance values, so it is hard to tell differences between cloud and no cloud classes. 

```{r,fig.width=4,fig.height=4, fig.cap="\\label{fig:figs}Heatmap for correlations between features"}
image = image %>%
  mutate(expert_label = factor(expert_label))
image_cor = round(cor(image[,4:11]),2)
get_upper_tri <- function(corr){
    corr[lower.tri(corr)]<- NA
    return(corr)
}
upper_tri = get_upper_tri(image_cor)
melted_image_corr <- melt(upper_tri, na.rm = TRUE)

heatmap <- ggplot(data = melted_image_corr, aes(Var2, Var1, fill = value))+
 geom_tile(color = "white")+
 scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
   midpoint = 0, limit = c(-1,1), space = "Lab", 
   name="Correlations") +
  theme_minimal()+ 
 theme(axis.text.x = element_text(angle = 45, vjust = 1, 
    size = 12, hjust = 1))+
 coord_fixed()

heatmap + 
geom_text(aes(Var2, Var1, label = value), color = "black", size = 4) +
theme(
  axis.title.x = element_blank(),
  axis.title.y = element_blank(),
  panel.grid.major = element_blank(),
  panel.border = element_blank(),
  panel.background = element_blank(),
  axis.ticks = element_blank(),
  legend.justification = c(1, 0),
  legend.position = c(0.6, 0.7),
  legend.direction = "horizontal")+
  guides(fill = guide_colorbar(barwidth = 7, barheight = 1,
                title.position = "top", title.hjust = 0.5))
```


```{r}
g1 = ggplot(image,aes(x=expert_label, y=NDAI))+
  geom_boxplot()
g2 = ggplot(image,aes(x=expert_label, y=SD))+
  geom_boxplot()
g3 = ggplot(image,aes(x=expert_label, y=CORR))+
  geom_boxplot()
g4 = ggplot(image,aes(x=expert_label, y=DF))+
  geom_boxplot()
g5 = ggplot(image,aes(x=expert_label, y=CF))+
  geom_boxplot()
g6 = ggplot(image,aes(x=expert_label, y=BF))+
  geom_boxplot()
g7 = ggplot(image,aes(x=expert_label, y=AF))+
  geom_boxplot()
g8 = ggplot(image,aes(x=expert_label, y=AN))+
  geom_boxplot()
```

```{r,fig.width=9,fig.height=3, fig.cap="\\label{fig:figs}Boxplots for NDAI, SD, CORR"}
grid.arrange(g1,g2,g3,ncol=3)
```


```{r,fig.width=9,fig.height=6, fig.cap="\\label{fig:figs}Boxplots for radiances with different angles"}
grid.arrange(g4,g5,g6,g7,g8,ncol=3)
```



# 2 Preparation

## (a)

### First Splitting method
For our first method, we want to use a simple and direct way to split our data. In order to have enough data for training and testing, we can use the data of two pictures as our training set so that the training set takes 66.6% of the entire data. Then, we only need to split the data of image3 into a validation and test set. Here, we set the ratio of the amount of data in three sets to be around 2:1:1, so validation set should take about 16.6% of the entire data and test set should take 16.6% of the entire data. We first set two images to be our training set, and split the remaining picture into two rectangles of equal size. Then, we can take one rectangle as our validation set and one as test set. In this method, we need to make sure that each rectangle we split should contain data from different classes, or be diversified. After checking the three images we plotted in 1(b), we decide to use the first two images as our training set, and split the third one. In the third image, we can set thresholds values for the y-axis, and divide the image into two rectangles according to the threshold value.

From the figure 5, we can find that our thresholds for y-axis divide the image into three rectangles containing data from different classes. We can pick the one on the top to be our test set, and the remaining two to be our validation set. For the training, validation and test dataset from method1, they will have 11 dimensions as the original datasets.

```{r, fig.width=5, fig.height=3, fig.cap="\\label{fig:figs}First split method"}
image3_filtered = image3 %>% filter(expert_label!=0) %>% mutate(expert_label=factor(expert_label))
y_max_p3 = max(image3_filtered$y)
y_min_p3 = min(image3_filtered$y)
lim = y_max_p3-(y_max_p3-y_min_p3)/2
ggplot(image3_filtered,aes(x,y,color=expert_label))+
  geom_point()+
  scale_color_manual(values=c("grey40","blue"))+
  labs(colour="expert_label")+
  geom_hline(yintercept=lim,color="red")
```


```{r}
train_method1 = rbind(image1,image2) %>% filter(expert_label!=0) %>% mutate(expert_label=factor(expert_label))
test_method1 = image3_filtered[image3_filtered$y>=lim,]
validation_method1 = image3_filtered[image3_filtered$y<lim,]
```

### Second Method
In the second method, we can use blocks to split the all the images and randomly select 60% blocks as our training set, 20% as validation set, and the remaining 20% as test set. We plan to divide each image into 100 blocks and then we will have totally 300 blocks. Then, we can divide those 300 blocks into train, validation, and test sets according to the ratio we set. For method2, we need to record the index of the block of each pixel, and the number of the picture where the pixel is originally located. Therefore, for the training, validation and test dataset from method2, they will have 13 dimensions. For both methods, we will delete unlabeled pixels after splitting the data because they are not useful for our classification. 


```{r}
set.seed(111)
image1_xcut = seq(min(image1$x),max(image1$x),length=11)
image2_xcut = seq(min(image2$x),max(image2$x),length=11)
image3_xcut = seq(min(image3$x),max(image3$x),length=11)
image1_ycut = seq(min(image1$y),max(image1$y),length=11)
image2_ycut = seq(min(image2$y),max(image2$y),length=11)
image3_ycut = seq(min(image3$y),max(image3$y),length=11)

blocks = list()
b_lab = 1
for(i in 1:10){
  for(j in 1:10){
    single_block = image1 %>% filter(x>image1_xcut[i] & x<image1_xcut[i+1] & y>image1_ycut[i] & y<image1_ycut[i+1]) %>% mutate(blocks=b_lab)
    blocks[[b_lab]] = single_block
    b_lab = b_lab+1
  }
}
for(i in 1:10){
  for(j in 1:10){
    single_block = image2 %>% filter(x>image2_xcut[i] & x<image2_xcut[i+1] & y>image2_ycut[i] & y<image2_ycut[i+1]) %>% mutate(blocks=b_lab)
    blocks[[b_lab]] = single_block
    b_lab = b_lab+1
  }
}
for(i in 1:10){
  for(j in 1:10){
    single_block = image3 %>% filter(x>image3_xcut[i] & x<image3_xcut[i+1] & y>image3_ycut[i] & y<image3_ycut[i+1]) %>% mutate(blocks=b_lab)
    blocks[[b_lab]] = single_block
    b_lab = b_lab+1
  }
}
sample_indexs = 1:300 
train_index = sample(sample_indexs,size = 180,replace = FALSE)
validation_index = sample(sample_indexs[-train_index],size=60,replace = FALSE)
test_index = sample_indexs[-c(train_index,validation_index)]

train_method2 = blocks[[train_index[1]]]
validation_method2 = blocks[[validation_index[1]]]
test_method2 = blocks[[test_index[1]]]
for(i in train_index[-1]){
  train_method2 = rbind(train_method2,blocks[[i]])
}
for(i in validation_index[-1]){
  validation_method2 = rbind(validation_method2,blocks[[i]])
}
for(i in test_index[-1]){
  test_method2 = rbind(test_method2,blocks[[i]])
}
train_method2 = train_method2%>%filter(expert_label!=0)%>%mutate(expert_label=factor(expert_label))
test_method2 = test_method2%>%filter(expert_label!=0)%>%mutate(expert_label=factor(expert_label))
validation_method2 = validation_method2%>%filter(expert_label!=0)%>%mutate(expert_label=factor(expert_label))
```


## (b)
The accuracy of the trivial classifier on the validation and test set of two methods is:
```{r}
# accuracy of method1
accuracy = matrix(NA,nrow=2,ncol=2,dimnames=list(c("validation","test"),c("method1","method2")))
accuracy[1,1] = sum(validation_method1$expert_label==-1)/nrow(validation_method1)
accuracy[1,2] = sum(validation_method2$expert_label==-1)/nrow(validation_method2)
accuracy[2,1] = sum(test_method1$expert_label==-1)/nrow(test_method1)
accuracy[2,2] = sum(test_method2$expert_label==-1)/nrow(test_method2)
accuracy
```

This kind of trivial classifier will have high accuracy when a dataset consist almost entirely of cloud-free class. On the table above, we can find that the trivial classifier does not perform well on most of the sets. For the validation set of our method1, since non-cloud class occupies a large proportion of the data, the accuracy of trivial classifier is pretty high compared with other sets, but the accuracy is still only around 0.75. Therefore, we would say our classification problem is not trivial.

## (c)
In order to determine 3 best features out of 8 features, we firstly used random forest to find and plot the importance of each feature based on train sets of both methods. We found that NDAI, SD and CORR were the features having the greatest importance in both plots. Then, we decided to use each feature to build a single-predictor logistics regression model based on training data set from both methods, and use the validation data sets to draw the ROC curve.
We got two area under the ROC curve (AUC) values, and made them equal weighted to get the average AUC value to evaluate the overall performance of each model. 
An ideal ROC curve will have huge top left corner, so the larger the average AUC value the better the classifier. Again, NDAI, SD and CORR had the largest average AUC values. So, combined with the result of random forest, we decided to use NDAI, SD, and CORR as our three "best" features. Among them, NDAI performed the best in both random forest and ROC curves, so it should be the most useful variable for our classification.

```{r}
# change expert_label's not cloudy label from -1 to 0
temp_train_method1=train_method1 %>% 
   mutate(expert_label = ifelse(expert_label == -1,0,1))

temp_train_method2=train_method2 %>%
  mutate(expert_label = ifelse(expert_label == -1,0,1))

temp_validation_method1=validation_method1 %>%
  mutate(expert_label = ifelse(expert_label == -1,0,1))

temp_validation_method2=validation_method2 %>%
  mutate(expert_label = ifelse(expert_label == -1,0,1))
```

```{r, fig.width=8, fig.height=4, fig.cap="\\label{fig:figs}Feature importance by random forest"}
par(mfrow=c(1,2))
cloud_RF1 = randomForest(as.factor(expert_label)~NDAI+SD+CORR+AF+BF+CF+DF+AN,data=temp_train_method1,ntree=300)
cloud_RF2 = randomForest(as.factor(expert_label)~NDAI+SD+CORR+AF+BF+CF+DF+AN,data=temp_train_method2,ntree=300)
varImpPlot(cloud_RF1)
varImpPlot(cloud_RF2)
```

```{r}
logm1_1=glm(expert_label~NDAI,data=temp_train_method1,family = "binomial")
logm1_2=glm(expert_label~SD,data=temp_train_method1,family = "binomial")
logm1_3=glm(expert_label~CORR,data=temp_train_method1,family = "binomial")
logm1_4=glm(expert_label~DF,data=temp_train_method1,family = "binomial")
logm1_5=glm(expert_label~CF,data=temp_train_method1,family = "binomial")
logm1_6=glm(expert_label~BF,data=temp_train_method1,family = "binomial")
logm1_7=glm(expert_label~AF,data=temp_train_method1,family = "binomial")
logm1_8=glm(expert_label~AN,data=temp_train_method1,family = "binomial")
```
```{r}
m1_auc=c()
prob=predict(logm1_1,temp_validation_method1,type='response')
m1_auc[1]=auc(temp_validation_method1$expert_label, prob)
prob=predict(logm1_2,temp_validation_method1,type='response')
m1_auc[2]=auc(temp_validation_method1$expert_label, prob)

prob=predict(logm1_3,temp_validation_method1,type='response')
m1_auc[3]=auc(temp_validation_method1$expert_label, prob)
prob=predict(logm1_4,temp_validation_method1,type='response')
m1_auc[4]=auc(temp_validation_method1$expert_label, prob)

prob=predict(logm1_5,temp_validation_method1,type='response')
m1_auc[5]=auc(temp_validation_method1$expert_label, prob)
prob=predict(logm1_6,temp_validation_method1,type='response')
m1_auc[6]=auc(temp_validation_method1$expert_label, prob)

prob=predict(logm1_7,temp_validation_method1,type='response')
m1_auc[7]=auc(temp_validation_method1$expert_label, prob)
prob=predict(logm1_8,temp_validation_method1,type='response')
m1_auc[8]=auc(temp_validation_method1$expert_label, prob)
```

```{r}
logm2_1=glm(expert_label~NDAI,data=temp_train_method2,family = "binomial")
logm2_2=glm(expert_label~SD,data=temp_train_method2,family = "binomial")
logm2_3=glm(expert_label~CORR,data=temp_train_method2,family = "binomial")
logm2_4=glm(expert_label~DF,data=temp_train_method2,family = "binomial")
logm2_5=glm(expert_label~CF,data=temp_train_method2,family = "binomial")
logm2_6=glm(expert_label~BF,data=temp_train_method2,family = "binomial")
logm2_7=glm(expert_label~AF,data=temp_train_method2,family = "binomial")
logm2_8=glm(expert_label~AN,data=temp_train_method2,family = "binomial")
```
```{r}
library(ROCR)
m2_auc=c()
prob=predict(logm2_1,temp_validation_method2,type='response')
m2_auc[1]=auc(temp_validation_method2$expert_label, prob)
prob=predict(logm2_2,temp_validation_method2,type='response')
m2_auc[2]=auc(temp_validation_method2$expert_label, prob)

prob=predict(logm2_3,temp_validation_method2,type='response')
m2_auc[3]=auc(temp_validation_method2$expert_label, prob)
prob=predict(logm2_4,temp_validation_method2,type='response')
m2_auc[4]=auc(temp_validation_method2$expert_label, prob)

prob=predict(logm2_5,temp_validation_method2,type='response')
m2_auc[5]=auc(temp_validation_method2$expert_label, prob)
prob=predict(logm2_6,temp_validation_method2,type='response')
m2_auc[6]=auc(temp_validation_method2$expert_label, prob)

prob=predict(logm2_7,temp_validation_method2,type='response')
m2_auc[7]=auc(temp_validation_method2$expert_label, prob)
prob=predict(logm2_8,temp_validation_method2,type='response')
m2_auc[8]=auc(temp_validation_method2$expert_label, prob)
```

```{r}
avg_auc=(m1_auc+m2_auc)/2
rounded_AUC = round(c(m1_auc,m2_auc,avg_auc),3)
tab <- matrix(rounded_AUC, ncol=3)
colnames(tab) <- c('m1_auc','m2_auc','avg_auc')
rownames(tab) <- c("NDAI","SD","CORR","DF","CF","BF","AF","AN")
tab=as.table(tab[order(tab[,3],decreasing = T),])
tab
```

# 3 Modeling

## (a)

### How to create folds.

Firstly, we need to divide our datasets into k-folds. Let assume we would like to divide 10 folds. Due to the spatial dependency, we cannot randomly assign each pixel to a fold. Therefore, for the dataset we got by method1, we could divide the combination of training and validation data into 10 blocks by their x-axis values as below. We would combine the data of each column and make it as a fold. Then, by this method, we got 10 folds for our training and validation data from method1. However, we should notice that when the number of folds is large, there might be some folds with only one class. Just like the division in figure7, some folds consist mainly of one class. Therefore, our number of folds should not be very large. We think 10 folds are acceptable here because each fold still consist of different classes, but it should not be larger than 10.
```{r,fig.width=8,fig.height=3, fig.cap="\\label{fig:figs}Divide folds for method1"}
cut1 = seq(min(image1$x),max(image1$x),length=11)[-c(1,11)]
cut2 = seq(min(image2$x),max(image2$x),length=11)[-c(1,11)]
cut3 = seq(min(image3$x),max(image3$x),length=11)[-c(1,11)]
cut_p1 = image1 %>% 
  filter(expert_label!=0) %>%
  mutate(expert_label=as.factor(expert_label))%>%
  ggplot(.,aes(x,y,color=expert_label))+
  geom_point(show.legend = FALSE)+
  geom_vline(xintercept = cut1, color="red")
cut_p2 = image2 %>% 
  filter(expert_label!=0) %>%
  mutate(expert_label=as.factor(expert_label))%>%
  ggplot(.,aes(x,y,color=expert_label))+
  geom_point(show.legend = FALSE)+
  geom_vline(xintercept = cut2, color="red")
cut_p3 = ggplot(validation_method1,aes(x,y,color=expert_label))+
  geom_point(show.legend = FALSE)+
  geom_vline(xintercept = cut2, color="red")
gg_legend <- ggplot(validation_method1,aes(x,y,color=expert_label)) + 
  geom_point()+
  theme(legend.position = "bottom")
shared_legend <- extract_legend(gg_legend)
grid.arrange(arrangeGrob(cut_p1,cut_p2,cut_p3,ncol=3), shared_legend,nrow = 2, heights = c(10, 1))
```

Then, for the training and validation data from method2, we could extract the index of the blocks in our data first. However, for some blocks, since they do not contain any labeled pixel, they might already be deleted, so we may not have 240 blocks as we expected. After that, we randomly divide all the blocks into 10 folds equally. Each fold should contain similar number of blocks. Therefore, we could get 10 folds as well. In order to make it more convenient for us to calculate, we should divide the fold for our training data before we use the CVmaster and give a fold number to each pixel in the training data. Also, we need to change the label of non-cloud class from -1 to 0 to satisfy our models.

### Model Assumptions 

After splitting the data, we can use the CVmaster function we wrote in the previous part to get the accuracies across folds with models including Logistic Regression, Naive Bayes, QDA, LDA, KNN and XGBoost. We decided to set the number of folds to be 10, and we tried each model in datasets from both split methods.
Now, let's talk about the assumptions of models we choose and whehter they are satisfied:
$\textbf{Logistic Regression}$: Firstly, we use a binary logistic regression model here, so it assumes our dependent variable, expert label, to be binary. Since our the expert label only have two distinct values, it is binary and thus it the first assumption is satisfied. In addition, Logistic Regression requires the observations to be independent. However, we know that our pixels have spatial dependency, so it does not satisfy the assumption of independence here. 
$\textbf{Naive Bayes}$: This model assumes independence among all the features. In Figure 2, we can find that the correlation between NDAI and SD are 0.63 which is not very small, so we may say the assumption is not satisfied here. 
$\textbf{Linear Discriminant Analysis (LDA) and Quadratic Discriminant Analysis (QDA)}$: LDA and QDA require the each variables to be independent which is not satisfied in our dataset. In addition, they require these features to be normally distributed but we find the assumption is still not met by drawing density plots of those features. (Due to the space limit, we won't show the density plots here.) 
$\textbf{K-Nearest Neighbors (KNN)}$: KNN model assumes that data points with close proximity are highly similar. Since our data points have spatial dependency, this assumption is satisfied here. For the value of parameter K in KNN model, we would set it to be 20 because the data are spatially dependent and 20 is large enough to give good boundaries.
$\textbf{eXtreme Gradient Boosting (XGBoost)}$: This model does not have important assumptions for the distribution or dependency of the variables of our data. Therefore, we could say XGBoost model is very useful for most classification problems, and it is applicable in our data here.

### CV accuracies

In the table below, we can get the accuracies across folds for different models in data from two split methods. In these two tables, we find that all of the models give better accuracy in data splitted by method2. This might be because the method2 split the two classes more evenly than method1. Also, among all of the models, XGBoost perform much better than all other models in both data. The average accuracy of XGBoost in data from method2 is even higher than 0.99. Besides, KNN is the second best model. Except these two models, the performances of other models are very close.

```{r}
#split folds for data from method1
image1_split = image1 %>% mutate(folds = as.integer((x-min(x)-1)/(max(x)-min(x))*10)+1)
image2_split = image2 %>% mutate(folds = as.integer((x-min(x)-1)/(max(x)-min(x))*10)+1)
image3_split = validation_method1 %>% mutate(folds = as.integer((x-min(x)-1)/(max(x)-min(x))*10)+1)
train_cv1 = rbind(image1_split,image2_split,image3_split)
train_cv1 = train_cv1 %>% 
  filter(expert_label!=0)%>%
  mutate(expert_label = ifelse(expert_label==-1,0,1)) %>%
  mutate_at(c("NDAI", "SD", "CORR"), ~(scale(.) %>% as.vector))
# scale features and select inputs
train_feature1 = train_cv1 %>% dplyr::select(NDAI,SD,CORR)
train_label1 = train_cv1 %>% dplyr::select(expert_label)
train_folds1 = train_cv1 %>% dplyr::select(folds)
```

```{r}
#split folds for data from method2
train_cv2 = rbind(train_method2,validation_method2)
train_cv2$new_blocks <- match(train_cv2$blocks, unique(train_cv2$blocks))
# number of folds
K=10
train_cv2 = train_cv2 %>% 
  mutate(folds = new_blocks%%K+1, expert_label = ifelse(expert_label==-1,0,1)) %>% 
  mutate_at(c("NDAI", "SD", "CORR"), ~(scale(.) %>% as.vector))
# scale the features
train_feature2 = train_cv2 %>% dplyr::select(NDAI,SD,CORR)
train_label2 = train_cv2 %>% dplyr::select(expert_label)
train_folds2 = train_cv2 %>% dplyr::select(folds)
```

```{r}
#try different models here
source("CVmaster.R", local = knitr::knit_global())
lr_result1 <- CVmaster(lr_model, train_feature1, train_label1, train_folds1)
nb_result1 <- CVmaster(nb_model, train_feature1, train_label1, train_folds1)
lda_result1 <- CVmaster(lda_model, train_feature1, train_label1, train_folds1)
qda_result1 <- CVmaster(qda_model, train_feature1, train_label1, train_folds1)
knn_result1 <- CVmaster(knn_model, train_feature1, train_label1, train_folds1)
xgb_result1 <- CVmaster(XGBoost_model, train_feature1, train_label1, train_folds1)

lr_result2 <- CVmaster(lr_model, train_feature2, train_label2, train_folds2)
nb_result2 <- CVmaster(nb_model, train_feature2, train_label2, train_folds2)
lda_result2 <- CVmaster(lda_model, train_feature2, train_label2, train_folds2)
qda_result2 <- CVmaster(qda_model, train_feature2, train_label2, train_folds2)
knn_result2 <- CVmaster(knn_model, train_feature2, train_label2, train_folds2)
xgb_result2 <- CVmaster(XGBoost_model, train_feature2, train_label2, train_folds2)
```

```{r}
# accuracy table for both datasets with different models
result_list1 <- list(lr_result1, nb_result1, lda_result1, qda_result1, knn_result1, xgb_result1)
result_table1 <- result_list1 %>% reduce(full_join, by='folds')
result_table1

result_list2 <- list(lr_result2, nb_result2, lda_result2, qda_result2, knn_result2, xgb_result2)
result_table2 <- result_list2 %>% reduce(full_join, by="folds")
result_table2
```
 
After getting the accuracies across folds with different models, we can now train the data again and fit our test data with different models. Then, we can calculate the test accuracy of different models in data from both split methods. From the result table below, we can notice that all of the models still perform better in data from method2. In addition, just like the accuracies from K-fold Cross Validation, XGBoost and KNN perform much better than the other models in both data, and the difference between these two models become smaller than that in the K-fold Cross Validation. 

```{r}
# test accuracy
test1 <- test_method1 %>% 
  mutate(expert_label=ifelse(expert_label==-1,0,1)) %>%
  mutate_at(c("NDAI", "SD", "CORR"), ~(scale(.) %>% as.vector))
test2 <- test_method2 %>% 
  mutate(expert_label=ifelse(expert_label==-1,0,1)) %>%
  mutate_at(c("NDAI", "SD", "CORR"), ~(scale(.) %>% as.vector))
train1 <- rbind(train_method1,validation_method1) %>%
  mutate(expert_label=ifelse(expert_label==-1,0,1)) %>%
  mutate_at(c("NDAI", "SD", "CORR"), ~(scale(.) %>% as.vector))
train2 <- rbind(train_method2,validation_method2) %>% 
  mutate(expert_label=ifelse(expert_label==-1,0,1)) %>%
  mutate_at(c("NDAI", "SD", "CORR"), ~(scale(.) %>% as.vector))
test_accuracy <- matrix(NA,ncol=2,nrow=6)
rownames(test_accuracy) <- c("Logistic Regression", "Naive Bayes", "QDA", "LDA", "KNN", "XGBoost")
colnames(test_accuracy) <- c("split method1", "split method2")
form <- formula("expert_label ~ NDAI+SD+CORR")

# Logistic Regression
lr_mod1 <- glm(form, data=train1,family = "binomial")
lr_pred1 <- predict(lr_mod1, test1, type="response")
pred = rep(0,length(lr_pred1))
pred[lr_pred1>0.5] = 1
test_accuracy[1,1] <- round(mean(pred==test1$expert_label),3)

lr_mod2 <- glm(form, data=train2,family = "binomial")
lr_pred2 <- predict(lr_mod2, test2, type="response")
pred = rep(0,length(lr_pred2))
pred[lr_pred2>0.5] = 1
test_accuracy[1,2] <- round(mean(pred==test2$expert_label),3)

# Naive Bayes
nb_mod1 <- naiveBayes(form, data=train1)
nb_pred1 = predict(nb_mod1, test1)
test_accuracy[2,1] = round(mean(nb_pred1==test1$expert_label),3)
nb_mod2 <- naiveBayes(form, data=train2)
nb_pred2 = predict(nb_mod2, test2)
test_accuracy[2,2] = round(mean(nb_pred2==test2$expert_label),3)

# QDA
qda_mod1 <- qda(form, data=train1)
qda_pred1 = predict(qda_mod1, test1)
test_accuracy[3,1] = round(mean(qda_pred1$class==test1$expert_label),3)

qda_mod2 <- qda(form, data=train2)
qda_pred2 = predict(qda_mod2, test2)
test_accuracy[3,2] = round(mean(qda_pred2$class==test2$expert_label),3)

# LDA
lda_mod1 <- lda(form, data=train1)
lda_pred1 = predict(lda_mod1, test1)
test_accuracy[4,1] = round(mean(lda_pred1$class==test1$expert_label),3)
lda_mod2 <- lda(form, data=train2)
lda_pred2 = predict(lda_mod2, test2)
test_accuracy[4,2] = round(mean(lda_pred2$class==test2$expert_label),3)

# KNN
knn_mod1 <- knn(train1[,c("NDAI","SD","CORR")], test1[,c("NDAI","SD","CORR")], train1$expert_label, k=20)
test_accuracy[5,1] <- round(mean(knn_mod1==test1$expert_label),3)
knn_mod2 <- knn(train2[,c("NDAI","SD","CORR")], test2[,c("NDAI","SD","CORR")], train2$expert_label, k=20)
test_accuracy[5,2] <- round(mean(knn_mod2==test2$expert_label),3)

# XGBoost
xgb_train = xgb.DMatrix(data = data.matrix(train1[,c("NDAI","SD","CORR")]), label = train1$expert_label)
xgb_test = xgb.DMatrix(data = data.matrix(test1[,c("NDAI","SD","CORR")]), label = test1$expert_label)
watchlist = list(train=xgb_train, test=xgb_test)
model = xgb.train(data = xgb_train, max.depth = 3, watchlist=watchlist, nrounds = 1000, verbose = 0)
final_rounds = as.integer(model$evaluation_log[which.min(test_rmse),1])
train_xgb1 = xgboost(data = xgb_train, max.depth = 3, nrounds = final_rounds, verbose = 0)
xgb_pred1 = predict(train_xgb1, xgb_test)
pred = rep(0,length(xgb_pred1))
pred[xgb_pred1>0.5] = 1
test_accuracy[6,1] <- round(mean(pred==test1$expert_label),3)

xgb_train2 = xgb.DMatrix(data = data.matrix(train2[,c("NDAI","SD","CORR")]), label = train2$expert_label)
xgb_test2 = xgb.DMatrix(data = data.matrix(test2[,c("NDAI","SD","CORR")]), label = test2$expert_label)
watchlist = list(train=xgb_train2, test=xgb_test2)
model = xgb.train(data = xgb_train2, max.depth = 3, watchlist=watchlist, nrounds = 1000, verbose = 0)
final_rounds = as.integer(model$evaluation_log[which.min(test_rmse),1])
train_xgb2 = xgboost(data = xgb_train2, max.depth = 3, nrounds = final_rounds, verbose = 0)
xgb_pred2 = predict(train_xgb2, xgb_test2)
pred = rep(0,length(xgb_pred2))
pred[xgb_pred2>0.5] = 1
test_accuracy[6,2] <- round(mean(pred==test2$expert_label),3)

test_accuracy
```

## 3(b)

The cutoff point is highlighted as a red point in each ROC curve plot.
In this project, we need to consider the cost of increasing true positive rate (fraction of cloudy sample that are classified as cloudy) and the cost of decreasing false positive rate (fraction of not cloudy sample that are classified as cloudy). This is the same as we need to control the cost of decreasing false negative rate (fraction of not cloudy sample that are classified as cloudy), and control the cost of decreasing false positive rate. In this project, because both classification error will lead to incorrectly temperature prediction, we assume the cost of decrease these two error are the same. Therefore, we decide the cutoff point is the point whose Euclidean distance from top left corner in ROC curve plot is minimum. The cutoff point is highlighted implies the probability threshold in classification. 

```{r}
euclidean=function(a, b){
  sqrt(sum((a - b)^2))
  }

library(e1071)
library(ROCR)
# Logistis regression
probs_lr <- predict(lr_mod2, test2, type="response")
pred_lr <- prediction(probs_lr, test2$expert_label)
perf_lr <- performance(pred_lr, measure='tpr', x.measure='fpr')
roc_lr <- data.frame(fpr=unlist(perf_lr@x.values), tpr=unlist(perf_lr@y.values))
roc_lr$method <- "logistic regression"

min_distance=100
lr_record_x=0
lr_record_y=0
for(i in 1:length(roc_lr$fpr)){
  current_distance=euclidean(c(roc_lr$fpr[i],roc_lr$tpr[i]),c(0,1))
  if (current_distance<min_distance){
    min_distance=current_distance
    lr_record_x=roc_lr$fpr[i]
    lr_record_y=roc_lr$tpr[i]
  }
}

p1=ggplot(data=rbind(roc_lr),aes(x=fpr, y=tpr, linetype=method)) + 
  geom_line() +
  geom_abline(intercept =0, slope = 1, linetype=2) +
  geom_point(aes(x=lr_record_x,y=lr_record_y),col="red")+
  ggtitle("roc curve of logistics regression")

# Naive Bayes
probs_nb <- predict(nb_mod2, test2, type="raw")
pred_nb <- prediction(probs_nb[, 2], test2$expert_label)
perf_nb <- performance(pred_nb, measure='tpr', x.measure='fpr')
roc_nb <- data.frame(fpr=unlist(perf_nb@x.values), tpr=unlist(perf_nb@y.values))
roc_nb$method <- "naive bayes"

min_distance=100
nb_record_x=0
nb_record_y=0
for(i in 1:length(roc_nb$fpr)){
  current_distance=euclidean(c(roc_nb$fpr[i],roc_nb$tpr[i]),c(0,1))
  if (current_distance<min_distance){
    min_distance=current_distance
    nb_record_x=roc_nb$fpr[i]
    nb_record_y=roc_nb$tpr[i]
  }
}

p2=ggplot(data=rbind(roc_nb),aes(x=fpr, y=tpr, linetype=method)) + 
  geom_line() +
  geom_abline(intercept =0, slope = 1, linetype=2) +
  geom_point(aes(x=nb_record_x,y=nb_record_y),col="red")+
  ggtitle("roc curve of naive bayes")

# QDA
probs_qda = predict(qda_mod2, test2)
pred_qda <- prediction(probs_qda$posterior[,2], test2$expert_label)
perf_qda <- performance(pred_qda, measure='tpr', x.measure='fpr')
roc_qda <- data.frame(fpr=unlist(perf_qda@x.values), tpr=unlist(perf_qda@y.values))
roc_qda$method <- "QDA"

min_distance=100
qda_record_x=0
qda_record_y=0
for(i in 1:length(roc_qda$fpr)){
  current_distance=euclidean(c(roc_qda$fpr[i],roc_qda$tpr[i]),c(0,1))
  if (current_distance<min_distance){
    min_distance=current_distance
    qda_record_x=roc_qda$fpr[i]
    qda_record_y=roc_qda$tpr[i]
  }
}
p3=ggplot(data=rbind(roc_qda),aes(x=fpr, y=tpr, linetype=method)) + 
  geom_line() +
  geom_abline(intercept =0, slope = 1, linetype=2) +
  geom_point(aes(x=qda_record_x,y=qda_record_y),col="red")+
  ggtitle("roc curve of qda")

# LDA
probs_lda = predict(lda_mod2, test2)
pred_lda <- prediction(probs_lda$posterior[,2], test2$expert_label)
perf_lda <- performance(pred_lda, measure='tpr', x.measure='fpr')
roc_lda <- data.frame(fpr=unlist(perf_lda@x.values), tpr=unlist(perf_lda@y.values))
roc_lda$method <- "LDA"

min_distance=100
lda_record_x=0
lda_record_y=0
for(i in 1:length(roc_lda$fpr)){
  current_distance=euclidean(c(roc_lda$fpr[i],roc_lda$tpr[i]),c(0,1))
  if (current_distance<min_distance){
    min_distance=current_distance
    lda_record_x=roc_lda$fpr[i]
    lda_record_y=roc_lda$tpr[i]
  }
}

p4=ggplot(data=rbind(roc_lda),aes(x=fpr, y=tpr, linetype=method)) + 
  geom_line() +
  geom_abline(intercept =0, slope = 1, linetype=2) +
  geom_point(aes(x=lda_record_x,y=lda_record_y),col="red")+
  ggtitle("roc curve of lda")


# KNN
#knn_mod2 <- knn(train2[,c("NDAI","SD","CORR")], test2[,c("NDAI","SD","CORR")], train2$expert_label, k=20)
knn_mod2 <- class::knn(train2[,c("NDAI","SD","CORR")],test2[,c("NDAI","SD","CORR")], train2$expert_label, k=20, prob=TRUE)
probs_knn <- attr(knn_mod2, "prob")
probs_knn <- 2*ifelse(knn_mod2 == "0", 1-probs_knn, probs_knn) - 1
pred_knn <- prediction(probs_knn, test2$expert_label)
perf_knn=performance(pred_knn, measure = "tpr",x.measure =  "fpr")
roc_knn <- data.frame(fpr=unlist(perf_knn@x.values), tpr=unlist(perf_knn@y.values))
roc_knn$method <- "KNN"

min_distance=100
knn_record_x=0
knn_record_y=0
for(i in 1:length(roc_knn$fpr)){
  current_distance=euclidean(c(roc_knn$fpr[i],roc_knn$tpr[i]),c(0,1))
  if (current_distance<min_distance){
    min_distance=current_distance
    knn_record_x=roc_knn$fpr[i]
    knn_record_y=roc_knn$tpr[i]
  }
}
p5=ggplot(data=rbind(roc_knn),aes(x=fpr, y=tpr, linetype=method)) + 
  geom_line() +
  geom_abline(intercept =0, slope = 1, linetype=2) +
  geom_point(aes(x=knn_record_x,y=knn_record_y),col="red")+
  ggtitle("roc curve of knn")


# XGBoost
probs_XGBoost = predict(train_xgb2, xgb_test2,type="response")
pred_XGBoost <- prediction(probs_XGBoost, test2$expert_label)
perf_XGBoost <- performance(pred_XGBoost, measure='tpr', x.measure='fpr')
roc_XGBoost <- data.frame(fpr=unlist(perf_XGBoost@x.values), tpr=unlist(perf_XGBoost@y.values))
roc_XGBoost$method <- "XGBoost"

min_distance=100
XGBoost_record_x=0
XGBoost_record_y=0
for(i in 1:length(roc_XGBoost$fpr)){
  current_distance=euclidean(c(roc_XGBoost$fpr[i],roc_XGBoost$tpr[i]),c(0,1))
  if (current_distance<min_distance){
    min_distance=current_distance
    XGBoost_record_x=roc_XGBoost$fpr[i]
    XGBoost_record_y=roc_XGBoost$tpr[i]
  }
}
p6=ggplot(data=rbind(roc_XGBoost),aes(x=fpr, y=tpr, linetype=method)) + 
  geom_line() +
  geom_abline(intercept =0, slope = 1, linetype=2) +
  geom_point(aes(x=XGBoost_record_x,y=XGBoost_record_y),col="red")+
  ggtitle("roc curve of XGBoost")

grid.arrange(p1,p2,p3,p4,p5,p6,nrow=3)


#ggplot(data=rbind(roc_lr,roc_nb,roc_qda,roc_lda,roc_knn,roc_XGBoost),aes(x=fpr, y=tpr, linetype=method, color=method)) + 
#  geom_line() +
#  geom_abline(intercept =0, slope = 1, linetype=2) +
#  theme(legend.position=c(0.8,0.2), legend.title=element_blank())
```


## (c)

In order to assess the fit of our model more accurately, we decided to find the F1 score of our models.
F1 score is a metric which is calculated as: $$F1 = 2 * \frac{Precision*Recall}{Precision+Recall}$$
A high F1 score means the false positives and false negatives of our model is pretty low. Therefore, the higher the F1 score, the better our model is.
```{r}
F1_table <- matrix(NA, ncol=2, nrow=6)
colnames(F1_table) <- c("split method1", "split method2")
rownames(F1_table) <- c("Logistic Regression", "Naive Bayes", "QDA", "LDA", "KNN", "XGBoost")

#logistic regression
pred = rep(0, length(test1$expert_label))
pred[lr_pred1>0.5] = 1
F1_table[1,1] <- confusionMatrix(as.factor(pred), as.factor(test1$expert_label))$byClass["F1"]
pred = rep(0, length(test2$expert_label))
pred[lr_pred2>0.5] = 1
F1_table[1,2] <- confusionMatrix(as.factor(pred), as.factor(test2$expert_label))$byClass["F1"]

#naive bayes
F1_table[2,1] <- confusionMatrix(as.factor(nb_pred1), as.factor(test1$expert_label))$byClass["F1"]
F1_table[2,2] <- confusionMatrix(as.factor(nb_pred2), as.factor(test2$expert_label))$byClass["F1"]

#QDA
F1_table[3,1] <- confusionMatrix(as.factor(qda_pred1$class), as.factor(test1$expert_label))$byClass["F1"]
F1_table[3,2] <- confusionMatrix(as.factor(qda_pred2$class), as.factor(test2$expert_label))$byClass["F1"]

#LDA
F1_table[4,1] <- confusionMatrix(as.factor(lda_pred1$class), as.factor(test1$expert_label))$byClass["F1"]
F1_table[4,2] <- confusionMatrix(as.factor(lda_pred2$class), as.factor(test2$expert_label))$byClass["F1"]

#KNN
F1_table[5,1] <- confusionMatrix(as.factor(knn_mod1), as.factor(test1$expert_label))$byClass["F1"]
F1_table[5,2] <- confusionMatrix(as.factor(knn_mod2), as.factor(test2$expert_label))$byClass["F1"]

#XGBoost
pred = rep(0,length(xgb_pred1))
pred[xgb_pred1>0.5] = 1
F1_table[6,1] <- confusionMatrix(as.factor(pred), as.factor(test1$expert_label))$byClass["F1"]
pred = rep(0,length(xgb_pred2))
pred[xgb_pred2>0.5] = 1
F1_table[6,2] <- confusionMatrix(as.factor(pred), as.factor(test2$expert_label))$byClass["F1"]

F1_table = as.data.frame(round(F1_table, 3))
F1_table
```

From the F1 score table above, we can see that KNN and XGBoost models have the highest F1 scores in both dataset, so they are still the best two models among all of the models we tried.


# 4 Diagnostics

## method 2
```{r}
xgb_train2 = xgb.DMatrix(data = data.matrix(train2[,c("NDAI","SD","CORR")]), label = train2$expert_label)
xgb_test2 = xgb.DMatrix(data = data.matrix(test2[,c("NDAI","SD","CORR")]), label = test2$expert_label)
watchlist = list(train=xgb_train2, test=xgb_test2)
```

## Adjust max.depth in xgboost to check parameter influence
```{r}
max_depth_accuracy=matrix(0,ncol=2,nrow=7)
colnames(max_depth_accuracy) <- c("max_depth", "test accuracy")

for(i in 2:8){
model = xgb.train(data = xgb_train2, max.depth = i, watchlist=watchlist, nrounds = 1000, verbose = 0)
final_rounds = as.integer(model$evaluation_log[which.min(test_rmse),1])
train_xgb2 = xgboost(data = xgb_train2, max.depth = i, nrounds = final_rounds, verbose = 0)
xgb_pred2 = predict(train_xgb2, xgb_test2)
pred = rep(0,length(xgb_pred2))
pred[xgb_pred2>0.5] = 1
max_depth_accuracy[i-1,2] <- round(mean(pred==test2$expert_label),3)
max_depth_accuracy[i-1,1]=i
}
max_depth_accuracy
```

## Adjust min_child_weight in xgboost to check parameter influence 
```{r}
min_child_weight_test_accuracy=matrix(0,ncol=2,nrow=6)
colnames(min_child_weight_test_accuracy) <- c("min_child_weight", "test accuracy")

for(i in 1:6){
model = xgb.train(data = xgb_train2, min_child_weight=i, watchlist=watchlist, nrounds = 1000, verbose = 0)
final_rounds = as.integer(model$evaluation_log[which.min(test_rmse),1])
train_xgb2 = xgboost(data = xgb_train2, min_child_weight=i, nrounds = final_rounds, verbose = 0)
xgb_pred2 = predict(train_xgb2, xgb_test2)
pred = rep(0,length(xgb_pred2))
pred[xgb_pred2>0.5] = 1
min_child_weight_test_accuracy[i,2] <- round(mean(pred==test2$expert_label),3)
min_child_weight_test_accuracy[i,1]=i
}
min_child_weight_test_accuracy
```

## Adjust eta in xgboost to check parameter influence
```{r}
eta_test_accuracy=matrix(0,ncol=2,nrow=5)
colnames(eta_test_accuracy) <- c("eta", "test accuracy")

for(i in 1:5){
model = xgb.train(data = xgb_train2, eta=i/10, watchlist=watchlist, nrounds = 1000, verbose = 0)
final_rounds = as.integer(model$evaluation_log[which.min(test_rmse),1])
train_xgb2 = xgboost(data = xgb_train2, eta=i/10, nrounds = final_rounds, verbose = 0)
xgb_pred2 = predict(train_xgb2, xgb_test2)
pred = rep(0,length(xgb_pred2))
pred[xgb_pred2>0.5] = 1
eta_test_accuracy[i,2] <- round(mean(pred==test2$expert_label),3)
eta_test_accuracy[i,1]=i/10
}
eta_test_accuracy
```

## Adjust subsample in xgboost to check parameter influence
```{r}
subsample_test_accuracy=matrix(0,ncol=2,nrow=6)
colnames(subsample_test_accuracy) <- c("subsample", "test accuracy")

for(i in 5:10){
model = xgb.train(data = xgb_train2, subsample=i/10, watchlist=watchlist, nrounds = 1000, verbose = 0)
final_rounds = as.integer(model$evaluation_log[which.min(test_rmse),1])
train_xgb2 = xgboost(data = xgb_train2, subsample=i/10, nrounds = final_rounds, verbose = 0)
xgb_pred2 = predict(train_xgb2, xgb_test2)
pred = rep(0,length(xgb_pred2))
pred[xgb_pred2>0.5] = 1
subsample_test_accuracy[i-4,2] <- round(mean(pred==test2$expert_label),3)
subsample_test_accuracy[i-4,1]=i/10
}
subsample_test_accuracy
```

## Adjust colsample_bynode in xgboost to check parameter influence
```{r}
colsample_bynode_test_accuracy=matrix(0,ncol=2,nrow=6)
colnames(colsample_bynode_test_accuracy) <- c("colsample_bynode", "test accuracy")

for(i in 5:10){
model = xgb.train(data = xgb_train2, colsample_bynode=i/10, watchlist=watchlist, nrounds = 1000, verbose = 0)
final_rounds = as.integer(model$evaluation_log[which.min(test_rmse),1])
train_xgb2 = xgboost(data = xgb_train2, colsample_bynode=i/10, nrounds = final_rounds, verbose = 0)
xgb_pred2 = predict(train_xgb2, xgb_test2)
pred = rep(0,length(xgb_pred2))
pred[xgb_pred2>0.5] = 1
colsample_bynode_test_accuracy[i-4,2] <- round(mean(pred==test2$expert_label),3)
colsample_bynode_test_accuracy[i-4,1]=i/10
}
colsample_bynode_test_accuracy
```
## (a) method 1
```{r}
xgb_train = xgb.DMatrix(data = data.matrix(train1[,c("NDAI","SD","CORR")]), label = train1$expert_label)
xgb_test = xgb.DMatrix(data = data.matrix(test1[,c("NDAI","SD","CORR")]), label = test1$expert_label)
watchlist = list(train=xgb_train, test=xgb_test)
```

## Adjust max.depth in xgboost to check parameter influence method 1
```{r}
max_depth_accuracy1=matrix(0,ncol=2,nrow=7)
colnames(max_depth_accuracy1) <- c("max_depth", "test accuracy")

for(i in 2:8){
model = xgb.train(data = xgb_train, max.depth = i, watchlist=watchlist, nrounds = 1000, verbose = 0)
final_rounds = as.integer(model$evaluation_log[which.min(test_rmse),1])
train_xgb1 = xgboost(data = xgb_train, max.depth = i, nrounds = final_rounds, verbose = 0)
xgb_pred1 = predict(train_xgb1, xgb_test)
pred = rep(0,length(xgb_pred1))
pred[xgb_pred1>0.5] = 1

max_depth_accuracy1[i-1,2] <- round(mean(pred==test1$expert_label),3)
max_depth_accuracy1[i-1,1]=i
}
max_depth_accuracy1
```
```{r}
xgb_train = xgb.DMatrix(data = data.matrix(train1[,c("NDAI","SD","CORR")]), label = train1$expert_label)
xgb_test = xgb.DMatrix(data = data.matrix(test1[,c("NDAI","SD","CORR")]), label = test1$expert_label)
watchlist = list(train=xgb_train, test=xgb_test)
```

## Adjust min_child_weight in xgboost to check parameter influence method 1
```{r}
min_child_weight_test_accuracy1=matrix(0,ncol=2,nrow=6)
colnames(min_child_weight_test_accuracy1) <- c("min_child_weight", "test accuracy")

for(i in 1:6){
model = xgb.train(data = xgb_train, min_child_weight = i, watchlist=watchlist, nrounds = 1000, verbose = 0)
final_rounds = as.integer(model$evaluation_log[which.min(test_rmse),1])
train_xgb1 = xgboost(data = xgb_train, min_child_weight = i, nrounds = final_rounds, verbose = 0)
xgb_pred1 = predict(train_xgb1, xgb_test)
pred = rep(0,length(xgb_pred1))
pred[xgb_pred1>0.5] = 1

min_child_weight_test_accuracy1[i,2] <- round(mean(pred==test1$expert_label),3)
min_child_weight_test_accuracy1[i,1]=i
}
min_child_weight_test_accuracy1
```

## Adjust eta in xgboost to check parameter influence method 1
```{r}
eta_test_accuracy1=matrix(0,ncol=2,nrow=5)
colnames(eta_test_accuracy1) <- c("eta", "test accuracy")

for(i in 1:5){
model = xgb.train(data = xgb_train, eta = i/10, watchlist=watchlist, nrounds = 1000, verbose = 0)
final_rounds = as.integer(model$evaluation_log[which.min(test_rmse),1])
train_xgb1 = xgboost(data = xgb_train, eta = i/10, nrounds = final_rounds, verbose = 0)
xgb_pred1 = predict(train_xgb1, xgb_test)
pred = rep(0,length(xgb_pred1))
pred[xgb_pred1>0.5] = 1

eta_test_accuracy1[i,2] <- round(mean(pred==test1$expert_label),3)
eta_test_accuracy1[i,1]=i
}
eta_test_accuracy1
```

## Adjust subsample in xgboost to check parameter influence method 1
```{r}
subsample_test_accuracy1=matrix(0,ncol=2,nrow=6)
colnames(subsample_test_accuracy1) <- c("subsample", "test accuracy")

for(i in 5:10){
model = xgb.train(data = xgb_train, subsample = i/10, watchlist=watchlist, nrounds = 1000, verbose = 0)
final_rounds = as.integer(model$evaluation_log[which.min(test_rmse),1])
train_xgb1 = xgboost(data = xgb_train, subsample = i/10, nrounds = final_rounds, verbose = 0)
xgb_pred1 = predict(train_xgb1, xgb_test)
pred = rep(0,length(xgb_pred1))
pred[xgb_pred1>0.5] = 1

subsample_test_accuracy1[i-4,2] <- round(mean(pred==test2$expert_label),3)
subsample_test_accuracy1[i-4,1]=i/10
}
subsample_test_accuracy1
```

## Adjust colsample_bynode in xgboost to check parameter influence method1
```{r}
colsample_bynode_test_accuracy1=matrix(0,ncol=2,nrow=6)
colnames(colsample_bynode_test_accuracy1) <- c("colsample_bynode", "test accuracy")

for(i in 5:10){
model = xgb.train(data = xgb_train, colsample_bynode = i/10, watchlist=watchlist, nrounds = 1000, verbose = 0)
final_rounds = as.integer(model$evaluation_log[which.min(test_rmse),1])
train_xgb1 = xgboost(data = xgb_train, colsample_bynode = i/10, nrounds = final_rounds, verbose = 0)
xgb_pred1 = predict(train_xgb1, xgb_test)
pred = rep(0,length(xgb_pred1))
pred[xgb_pred1>0.5] = 1

colsample_bynode_test_accuracy1[i-4,2] <- round(mean(pred==test2$expert_label),3)
colsample_bynode_test_accuracy1[i-4,1]=i/10
}
colsample_bynode_test_accuracy1
```

The best model (method 1) have following parameter:
max.depth = 3,
min_child_weight=1,
eta=0.3,
subsample=0.6,
colsample_bynode=0.6
```{r}
xgb_train = xgb.DMatrix(data = data.matrix(train1[,c("NDAI","SD","CORR")]), label = train1$expert_label)
xgb_test = xgb.DMatrix(data = data.matrix(test1[,c("NDAI","SD","CORR")]), label = test1$expert_label)
watchlist = list(train=xgb_train, test=xgb_test)

model = xgb.train(data = xgb_train, max.depth = 3,min_child_weight=1, eta=0.3,subsample=0.6,colsample_bynode=0.6,watchlist=watchlist, nrounds = 1000, verbose = 0)
final_rounds = as.integer(model$evaluation_log[which.min(test_rmse),1])
train_xgb1 = xgboost(data = xgb_train, max.depth = 3,min_child_weight=1, eta=0.3,subsample=0.6,colsample_bynode=0.6, nrounds = final_rounds, verbose = 0)
xgb_pred1 = predict(train_xgb1, xgb_test)
pred = rep(0,length(xgb_pred1))
pred[xgb_pred1>0.5] = 1
round(mean(pred==test1$expert_label),3)
```


The best model (method 2) have following parameter:
max.depth = 6,
min_child_weight=1,
eta=0.1,
subsample=1,
colsample_bynode=0.8
```{r}
# best model after adjustment
xgb_train2 = xgb.DMatrix(data = data.matrix(train2[,c("NDAI","SD","CORR")]), label = train2$expert_label)
xgb_test2 = xgb.DMatrix(data = data.matrix(test2[,c("NDAI","SD","CORR")]), label = test2$expert_label)
watchlist = list(train=xgb_train2, test=xgb_test2)

model = xgb.train(data = xgb_train2, max.depth = 6,min_child_weight=1, eta=0.1,subsample=1,colsample_bynode=0.8,watchlist=watchlist, nrounds = 1000, verbose = 0)

final_rounds = as.integer(model$evaluation_log[which.min(test_rmse),1])
train_xgb2 = xgboost(data = xgb_train2, max.depth = 6,min_child_weight=1, eta=0.1,subsample=1,colsample_bynode=0.8,nrounds = final_rounds, verbose = 0)
xgb_pred2 = predict(train_xgb2, xgb_test2)
pred = rep(0,length(xgb_pred2))
pred[xgb_pred2>0.5] = 1

round(mean(pred==test2$expert_label),3)
```



```{r}
# 用3a得到的model,先不调整参数，在原图里highlight missclassification point
# XGBoost
# method 1
#xgb_train = xgb.DMatrix(data = data.matrix(train1[,c("NDAI","SD","CORR")]), label = train1$expert_label)
#xgb_test = xgb.DMatrix(data = data.matrix(test1[,c("NDAI","SD","CORR")]), label = test1$expert_label)
#watchlist = list(train=xgb_train, test=xgb_test)
#model = xgb.train(data = xgb_train, max.depth = 3, watchlist=watchlist, nrounds = 1000, verbose = 0)
#final_rounds = as.integer(model$evaluation_log[which.min(test_rmse),1])
#train_xgb1 = xgboost(data = xgb_train, max.depth = 3, nrounds = final_rounds, verbose = 0)
xgb_pred1 = predict(train_xgb1, xgb_test)
pred = rep(0,length(xgb_pred1))
pred[xgb_pred1>0.5] = 1

miss_x_method1=c()
miss_y_method1=c()
miss_image_method1=c()
n=length(test1$expert_label)
for(i in 1:n){
  if (pred[i]!=test1$expert_label[i]){
    miss_x_method1=c(miss_x_method1,test1$x[i])
    miss_y_method1=c(miss_y_method1,test1$y[i])
    miss_image_method1=c(miss_image_method1,test1$image_num[i])
  }
}
miss_method1_df=data.frame(miss_x_method1,miss_y_method1,miss_image_method1)
miss_method1_df_image1=subset(miss_method1_df,miss_image_method2==1)
miss_method1_df_image2=subset(miss_method1_df,miss_image_method2==2)
miss_method1_df_image3=subset(miss_method1_df,miss_image_method2==3)



# methdod 2
#xgb_train2 = xgb.DMatrix(data = data.matrix(train2[,c("NDAI","SD","CORR")]), label = train2$expert_label)
#xgb_test2 = xgb.DMatrix(data = data.matrix(test2[,c("NDAI","SD","CORR")]), label = test2$expert_label)
#watchlist = list(train=xgb_train2, test=xgb_test2)
#model = xgb.train(data = xgb_train2, max.depth = 3, watchlist=watchlist, nrounds = 1000, verbose = 0)
#final_rounds = as.integer(model$evaluation_log[which.min(test_rmse),1])
#train_xgb2 = xgboost(data = xgb_train2, max.depth = 3, nrounds = final_rounds, verbose = 0)
xgb_pred2 = predict(train_xgb2, xgb_test2)
pred = rep(0,length(xgb_pred2))
pred[xgb_pred2>0.5] = 1

miss_x_method2=c()
miss_y_method2=c()
miss_image_method2=c()
n=length(test2$expert_label)
for(i in 1:n){
  if (pred[i]!=test2$expert_label[i]){
    miss_x_method2=c(miss_x_method2,test2$x[i])
    miss_y_method2=c(miss_y_method2,test2$y[i])
    miss_image_method2=c(miss_image_method2,test2$image_num[i])
  }
}
miss_method2_df=data.frame(miss_x_method2,miss_y_method2,miss_image_method2)
miss_method2_df_image1=subset(miss_method2_df,miss_image_method2==1)
miss_method2_df_image2=subset(miss_method2_df,miss_image_method2==2)
miss_method2_df_image3=subset(miss_method2_df,miss_image_method2==3)
```

# method 1 missclassification points location
```{r}
myColors = c("gray40","black","white","red")
p1 = ggplot() + 
  geom_point(data=image1, aes(x, y, colour = factor(expert_label)))+
  scale_color_manual(values=myColors)+
  geom_point(data=miss_method1_df_image1,mapping=aes(x = miss_x_method1, y = miss_y_method1, color = "misclassification"))+
  labs(color="exper_label")+ theme(legend.position = "none")

p2 = ggplot() + 
  geom_point(data=image2, aes(x, y, colour = factor(expert_label)))+
  scale_color_manual(values=myColors)+
  geom_point(data=miss_method1_df_image2,mapping=aes(x = miss_x_method1, y = miss_y_method1, color = "misclassification"))+
  labs(color="expert_label")+ theme(legend.position = "none")

p3 = ggplot() + 
  geom_point(data=image3, aes(x, y, colour = factor(expert_label)))+
  scale_color_manual(values=myColors)+
  geom_point(data=miss_method1_df_image3,mapping=aes(x = miss_x_method1, y = miss_y_method1, color = "misclassification"))+
  labs(colour="expert_label")+ theme(legend.position = "none")
```

```{r,fig.width=8,fig.height=3}
gg_legend <- ggplot() + 
  geom_point(data=image1, aes(x, y, colour = factor(expert_label)))+
  scale_color_manual(values=myColors)+
  geom_point(data=miss_method1_df_image1,mapping=aes(x = miss_x_method1, y = miss_y_method1, color = "misclassification"))+
  labs(colour="expert_label") +
  theme(legend.position = "bottom")
shared_legend <- extract_legend(gg_legend)
g=arrangeGrob(arrangeGrob(p1,p2,p3,ncol=3), shared_legend,nrow = 2, heights = c(10, 1))
ggsave("method1_missclassification.png",g,height=3,width = 8,dpi=600)
```

# method 2 missclassification points location
```{r}
myColors = c("gray40","black","white","red")
p1 = ggplot() + 
  geom_point(data=image1, aes(x, y, colour = factor(expert_label)))+
  scale_color_manual(values=myColors)+
  geom_point(data=miss_method2_df_image1,mapping=aes(x = miss_x_method2, y = miss_y_method2, color = "misclassification"))+
  labs(color="exper_label")+ theme(legend.position = "none")

p2 = ggplot() + 
  geom_point(data=image2, aes(x, y, colour = factor(expert_label)))+
  scale_color_manual(values=myColors)+
  geom_point(data=miss_method2_df_image2,mapping=aes(x = miss_x_method2, y = miss_y_method2, color = "misclassification"))+
  labs(color="expert_label")+ theme(legend.position = "none")

p3 = ggplot() + 
  geom_point(data=image3, aes(x, y, colour = factor(expert_label)))+
  scale_color_manual(values=myColors)+
  geom_point(data=miss_method2_df_image3,mapping=aes(x = miss_x_method2, y = miss_y_method2, color = "misclassification"))+
  labs(colour="expert_label")+ theme(legend.position = "none")
```

```{r,fig.width=8,fig.height=3}
gg_legend <- ggplot() + 
  geom_point(data=image1, aes(x, y, colour = factor(expert_label)))+
  scale_color_manual(values=myColors)+
  geom_point(data=miss_method2_df_image1,mapping=aes(x = miss_x_method2, y = miss_y_method2, color = "misclassification"))+
  labs(colour="expert_label") +
  theme(legend.position = "bottom")

shared_legend <- extract_legend(gg_legend)
g=arrangeGrob(arrangeGrob(p1,p2,p3,ncol=3), shared_legend,nrow = 2, heights = c(10, 1))
ggsave("method2_missclassification.png",g,height=3,width = 8,dpi=600)
```




# Acknowledgement

***
